{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4445 tokens long, so generating 1 tokens requires a sequence length of 4446, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4445 tokens long, so generating 1 tokens requires a sequence length of 4446, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4445 tokens long, so generating 1 tokens requires a sequence length of 4446, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4445 tokens long, so generating 1 tokens requires a sequence length of 4446, but the maximum supported sequence length is just 4096!\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Model: Meta-Llama-3.2-1B-Instruct, Prompting Method: direct\n",
      "Accuracy: 0.23, Cost: $0.01\n",
      "Results saved to nyt_location_result\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import openai\n",
    "from openai import RateLimitError, APIError\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.environ.get(\"SAMBANOVA_API_KEY\"),\n",
    "    base_url=\"https://api.sambanova.ai/v1\",\n",
    ")\n",
    "\n",
    "# Token counting using a simple string split method\n",
    "def count_tokens(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# Truncate text to fit within a specified token budget\n",
    "def truncate_text(text, max_tokens, reserve_tokens=50):\n",
    "    tokens = text.split()\n",
    "    max_input_tokens = max_tokens - reserve_tokens\n",
    "    if len(tokens) > max_input_tokens:\n",
    "        tokens = tokens[:max_input_tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Load the NYT dataset\n",
    "def load_nyt_dataset(data_dir='data/nyt_data'):\n",
    "    # File paths\n",
    "    phrase_file = os.path.join(data_dir, 'phrase_text.txt')\n",
    "    locations_file = os.path.join(data_dir, 'locations.txt')\n",
    "    locations_label_file = os.path.join(data_dir, 'locations_label.txt')\n",
    "\n",
    "    # Load phrases\n",
    "    with open(phrase_file, 'r', encoding='utf-8') as f:\n",
    "        phrases = [line.strip() for line in f]\n",
    "\n",
    "    # Load locations and their labels\n",
    "    with open(locations_file, 'r', encoding='utf-8') as f:\n",
    "        locations = [line.strip() for line in f]\n",
    "    with open(locations_label_file, 'r', encoding='utf-8') as f:\n",
    "        location_labels = [int(line.strip()) for line in f]\n",
    "\n",
    "    # Map location labels to location names\n",
    "    location_mapping = {i: name for i, name in enumerate(locations)}\n",
    "\n",
    "    # Combine data into a structured dictionary\n",
    "    data = []\n",
    "    for i, phrase in enumerate(phrases):\n",
    "        location = location_mapping.get(location_labels[i], \"Unknown\")\n",
    "        data.append({\n",
    "            'text': phrase,\n",
    "            'location': location,\n",
    "        })\n",
    "\n",
    "    return data, locations\n",
    "\n",
    "# Select random samples for evaluation\n",
    "def select_samples(data, num_samples_per_class=10):\n",
    "    classes = set(item['location'] for item in data)\n",
    "    selected_data = {cls: [] for cls in classes}\n",
    "\n",
    "    for item in data:\n",
    "        if len(selected_data[item['location']]) < num_samples_per_class:\n",
    "            selected_data[item['location']].append(item)\n",
    "\n",
    "    return selected_data\n",
    "\n",
    "# Prompting methods\n",
    "def direct_prompt(text, classes_formatted):\n",
    "    return (\n",
    "        f\"{text}\\n\\n\"\n",
    "        f\"Classify the above text into one of the following locations: {classes_formatted}.\\n\"\n",
    "        f\"Provide only the location name as your answer.\"\n",
    "    )\n",
    "\n",
    "def chain_of_thought_prompt(text, classes_formatted):\n",
    "    return (\n",
    "        f\"{text}\\n\\n\"\n",
    "        f\"Think step by step to classify the above text into one of the following locations: {classes_formatted}.\\n\"\n",
    "        f\"Provide only the location name as your answer.\"\n",
    "    )\n",
    "\n",
    "def few_shot_prompt(text, examples, classes_formatted):\n",
    "    examples_text = \"\"\n",
    "    for ex_text, ex_label in examples:\n",
    "        examples_text += f\"Text: {ex_text}\\nLabel: {ex_label}\\n\\n\"\n",
    "    return (\n",
    "        f\"{examples_text}\"\n",
    "        f\"Now, classify the following text:\\n\"\n",
    "        f\"Text: {text}\\n\\n\"\n",
    "        f\"Choose the location from the following options: {classes_formatted}.\\n\"\n",
    "        f\"Provide only the location name as your answer.\"\n",
    "    )\n",
    "\n",
    "# Cost estimation with actual prices\n",
    "def estimate_cost(input_tokens, output_tokens, model_name):\n",
    "    cost_per_million = {\n",
    "        'Llama 3.1 8B': (0.10, 0.20),\n",
    "        'Llama 3.1 70B': (0.60, 1.20),\n",
    "        'Llama 3.1 405B': (5.00, 10.00),\n",
    "        'Llama 3.2 1B': (0.04, 0.08),\n",
    "        'Llama 3.2 3B': (0.08, 0.16),\n",
    "        'Llama 3.2 11B Vision': (0.15, 0.30),\n",
    "        'Llama 3.2 90B Vision': (0.80, 1.60),\n",
    "    }\n",
    "    input_price, output_price = cost_per_million.get(model_name, (0.10, 0.20))\n",
    "    total_input_cost = (input_tokens / 1e6) * input_price\n",
    "    total_output_cost = (output_tokens / 1e6) * output_price\n",
    "    return total_input_cost + total_output_cost\n",
    "\n",
    "# Evaluate a single method and model\n",
    "def evaluate_model(\n",
    "    data, locations, model_name, prompting_method,\n",
    "    num_samples_per_class=10, token_budget=4000, delay=2, max_retries=3\n",
    "):\n",
    "    correct_predictions = 0\n",
    "    total_cost = 0\n",
    "    total_samples = 0\n",
    "    classes_formatted = \", \".join(locations)\n",
    "\n",
    "    selected_data = select_samples(data, num_samples_per_class)\n",
    "\n",
    "    for cls, samples in selected_data.items():\n",
    "        for item in samples:\n",
    "            # Truncate text to fit within token budget\n",
    "            text = truncate_text(item['text'], token_budget, reserve_tokens=100)\n",
    "\n",
    "            # Create prompt based on the chosen prompting method\n",
    "            if prompting_method == 'direct':\n",
    "                prompt = direct_prompt(text, classes_formatted)\n",
    "            elif prompting_method == 'chain-of-thought':\n",
    "                prompt = chain_of_thought_prompt(text, classes_formatted)\n",
    "            elif prompting_method == 'few-shot':\n",
    "                examples = random.sample(data, min(3, len(data)))\n",
    "                examples = [(ex['text'], ex['location']) for ex in examples]\n",
    "                prompt = few_shot_prompt(text, examples, classes_formatted)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown prompting method: {prompting_method}\")\n",
    "\n",
    "            input_token_count = count_tokens(prompt)\n",
    "            if input_token_count > token_budget:\n",
    "                print(f\"Skipping sample due to token limit: {input_token_count} tokens\")\n",
    "                continue\n",
    "\n",
    "            retries = 0\n",
    "            while retries <= max_retries:\n",
    "                try:\n",
    "                    # API call\n",
    "                    completion = client.chat.completions.create(\n",
    "                        model=model_name,\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                        stream=True,\n",
    "                        temperature=0.1,\n",
    "                        top_p=0.1\n",
    "                    )\n",
    "                    response_text = \"\".join(\n",
    "                        chunk.choices[0].delta.content or \"\" for chunk in completion\n",
    "                    )\n",
    "\n",
    "                    output_token_count = count_tokens(response_text)\n",
    "                    total_cost += estimate_cost(input_token_count, output_token_count, model_name)\n",
    "\n",
    "                    if response_text.strip().lower() == cls.lower():\n",
    "                        correct_predictions += 1\n",
    "                    break  # Exit retry loop on success\n",
    "\n",
    "                except RateLimitError:\n",
    "                    print(\"Rate limit exceeded. Sleeping for 30 seconds...\")\n",
    "                    time.sleep(30)\n",
    "                    retries += 1\n",
    "\n",
    "                except APIError as e:\n",
    "                    print(f\"API Error: {e}\")\n",
    "                    if \"maximum sequence length\" in str(e):\n",
    "                        print(\"Token limit exceeded. Please adjust the input size.\")\n",
    "                    retries += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Unexpected error: {e}\")\n",
    "                    break\n",
    "\n",
    "            total_samples += 1\n",
    "            time.sleep(delay)  # Respect delay between requests\n",
    "\n",
    "    accuracy = correct_predictions / total_samples if total_samples > 0 else 0\n",
    "    return accuracy, total_cost\n",
    "\n",
    "# Save results to a CSV file\n",
    "def save_results_to_csv(file_path, model_name, prompting_method, accuracy, cost):\n",
    "    # Check if the file exists\n",
    "    file_exists = os.path.isfile(file_path)\n",
    "\n",
    "    # Create a DataFrame with the current results\n",
    "    result = pd.DataFrame([{\n",
    "        'model': model_name,\n",
    "        'prompting_method': prompting_method,\n",
    "        'accuracy': accuracy,\n",
    "        'cost': cost\n",
    "    }])\n",
    "\n",
    "    # Append to the file or create a new one\n",
    "    if file_exists:\n",
    "        # Append without writing the header again\n",
    "        result.to_csv(file_path, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        # Write with header if the file is new\n",
    "        result.to_csv(file_path, index=False)\n",
    "\n",
    "# Define your model and method here\n",
    "model_name = \"Meta-Llama-3.2-1B-Instruct\"  # Change to the desired model\n",
    "prompting_method = \"direct\"  # Change to 'direct', 'chain-of-thought', or 'few-shot'\n",
    "\n",
    "# Load dataset and evaluate\n",
    "data, locations = load_nyt_dataset()\n",
    "accuracy, cost = evaluate_model(data, locations, model_name, prompting_method)\n",
    "\n",
    "# Print and save results\n",
    "print(f\"Model: {model_name}, Prompting Method: {prompting_method}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}, Cost: ${cost:.2f}\")\n",
    "\n",
    "results_file = 'nyt_location_result'\n",
    "save_results_to_csv(results_file, model_name, prompting_method, accuracy, cost)\n",
    "print(f\"Results saved to {results_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4449 tokens long, so generating 1 tokens requires a sequence length of 4450, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4449 tokens long, so generating 1 tokens requires a sequence length of 4450, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4449 tokens long, so generating 1 tokens requires a sequence length of 4450, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4449 tokens long, so generating 1 tokens requires a sequence length of 4450, but the maximum supported sequence length is just 4096!\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Model: Meta-Llama-3.2-1B-Instruct, Prompting Method: chain-of-thought\n",
      "Accuracy: 0.31, Cost: $0.01\n",
      "Results saved to nyt_location_result\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import openai\n",
    "from openai import RateLimitError, APIError\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.environ.get(\"SAMBANOVA_API_KEY\"),\n",
    "    base_url=\"https://api.sambanova.ai/v1\",\n",
    ")\n",
    "\n",
    "# Token counting using a simple string split method\n",
    "def count_tokens(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# Truncate text to fit within a specified token budget\n",
    "def truncate_text(text, max_tokens, reserve_tokens=50):\n",
    "    tokens = text.split()\n",
    "    max_input_tokens = max_tokens - reserve_tokens\n",
    "    if len(tokens) > max_input_tokens:\n",
    "        tokens = tokens[:max_input_tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Load the NYT dataset\n",
    "def load_nyt_dataset(data_dir='data/nyt_data'):\n",
    "    # File paths\n",
    "    phrase_file = os.path.join(data_dir, 'phrase_text.txt')\n",
    "    locations_file = os.path.join(data_dir, 'locations.txt')\n",
    "    locations_label_file = os.path.join(data_dir, 'locations_label.txt')\n",
    "\n",
    "    # Load phrases\n",
    "    with open(phrase_file, 'r', encoding='utf-8') as f:\n",
    "        phrases = [line.strip() for line in f]\n",
    "\n",
    "    # Load locations and their labels\n",
    "    with open(locations_file, 'r', encoding='utf-8') as f:\n",
    "        locations = [line.strip() for line in f]\n",
    "    with open(locations_label_file, 'r', encoding='utf-8') as f:\n",
    "        location_labels = [int(line.strip()) for line in f]\n",
    "\n",
    "    # Map location labels to location names\n",
    "    location_mapping = {i: name for i, name in enumerate(locations)}\n",
    "\n",
    "    # Combine data into a structured dictionary\n",
    "    data = []\n",
    "    for i, phrase in enumerate(phrases):\n",
    "        location = location_mapping.get(location_labels[i], \"Unknown\")\n",
    "        data.append({\n",
    "            'text': phrase,\n",
    "            'location': location,\n",
    "        })\n",
    "\n",
    "    return data, locations\n",
    "\n",
    "# Select random samples for evaluation\n",
    "def select_samples(data, num_samples_per_class=10):\n",
    "    classes = set(item['location'] for item in data)\n",
    "    selected_data = {cls: [] for cls in classes}\n",
    "\n",
    "    for item in data:\n",
    "        if len(selected_data[item['location']]) < num_samples_per_class:\n",
    "            selected_data[item['location']].append(item)\n",
    "\n",
    "    return selected_data\n",
    "\n",
    "# Prompting methods\n",
    "def direct_prompt(text, classes_formatted):\n",
    "    return (\n",
    "        f\"{text}\\n\\n\"\n",
    "        f\"Classify the above text into one of the following locations: {classes_formatted}.\\n\"\n",
    "        f\"Provide only the location name as your answer.\"\n",
    "    )\n",
    "\n",
    "def chain_of_thought_prompt(text, classes_formatted):\n",
    "    return (\n",
    "        f\"{text}\\n\\n\"\n",
    "        f\"Think step by step to classify the above text into one of the following locations: {classes_formatted}.\\n\"\n",
    "        f\"Provide only the location name as your answer.\"\n",
    "    )\n",
    "\n",
    "def few_shot_prompt(text, examples, classes_formatted):\n",
    "    examples_text = \"\"\n",
    "    for ex_text, ex_label in examples:\n",
    "        examples_text += f\"Text: {ex_text}\\nLabel: {ex_label}\\n\\n\"\n",
    "    return (\n",
    "        f\"{examples_text}\"\n",
    "        f\"Now, classify the following text:\\n\"\n",
    "        f\"Text: {text}\\n\\n\"\n",
    "        f\"Choose the location from the following options: {classes_formatted}.\\n\"\n",
    "        f\"Provide only the location name as your answer.\"\n",
    "    )\n",
    "\n",
    "# Cost estimation with actual prices\n",
    "def estimate_cost(input_tokens, output_tokens, model_name):\n",
    "    cost_per_million = {\n",
    "        'Llama 3.1 8B': (0.10, 0.20),\n",
    "        'Llama 3.1 70B': (0.60, 1.20),\n",
    "        'Llama 3.1 405B': (5.00, 10.00),\n",
    "        'Llama 3.2 1B': (0.04, 0.08),\n",
    "        'Llama 3.2 3B': (0.08, 0.16),\n",
    "        'Llama 3.2 11B Vision': (0.15, 0.30),\n",
    "        'Llama 3.2 90B Vision': (0.80, 1.60),\n",
    "    }\n",
    "    input_price, output_price = cost_per_million.get(model_name, (0.10, 0.20))\n",
    "    total_input_cost = (input_tokens / 1e6) * input_price\n",
    "    total_output_cost = (output_tokens / 1e6) * output_price\n",
    "    return total_input_cost + total_output_cost\n",
    "\n",
    "# Evaluate a single method and model\n",
    "def evaluate_model(\n",
    "    data, locations, model_name, prompting_method,\n",
    "    num_samples_per_class=10, token_budget=4000, delay=2, max_retries=3\n",
    "):\n",
    "    correct_predictions = 0\n",
    "    total_cost = 0\n",
    "    total_samples = 0\n",
    "    classes_formatted = \", \".join(locations)\n",
    "\n",
    "    selected_data = select_samples(data, num_samples_per_class)\n",
    "\n",
    "    for cls, samples in selected_data.items():\n",
    "        for item in samples:\n",
    "            # Truncate text to fit within token budget\n",
    "            text = truncate_text(item['text'], token_budget, reserve_tokens=100)\n",
    "\n",
    "            # Create prompt based on the chosen prompting method\n",
    "            if prompting_method == 'direct':\n",
    "                prompt = direct_prompt(text, classes_formatted)\n",
    "            elif prompting_method == 'chain-of-thought':\n",
    "                prompt = chain_of_thought_prompt(text, classes_formatted)\n",
    "            elif prompting_method == 'few-shot':\n",
    "                examples = random.sample(data, min(3, len(data)))\n",
    "                examples = [(ex['text'], ex['location']) for ex in examples]\n",
    "                prompt = few_shot_prompt(text, examples, classes_formatted)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown prompting method: {prompting_method}\")\n",
    "\n",
    "            input_token_count = count_tokens(prompt)\n",
    "            if input_token_count > token_budget:\n",
    "                print(f\"Skipping sample due to token limit: {input_token_count} tokens\")\n",
    "                continue\n",
    "\n",
    "            retries = 0\n",
    "            while retries <= max_retries:\n",
    "                try:\n",
    "                    # API call\n",
    "                    completion = client.chat.completions.create(\n",
    "                        model=model_name,\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                        stream=True,\n",
    "                        temperature=0.1,\n",
    "                        top_p=0.1\n",
    "                    )\n",
    "                    response_text = \"\".join(\n",
    "                        chunk.choices[0].delta.content or \"\" for chunk in completion\n",
    "                    )\n",
    "\n",
    "                    output_token_count = count_tokens(response_text)\n",
    "                    total_cost += estimate_cost(input_token_count, output_token_count, model_name)\n",
    "\n",
    "                    if response_text.strip().lower() == cls.lower():\n",
    "                        correct_predictions += 1\n",
    "                    break  # Exit retry loop on success\n",
    "\n",
    "                except RateLimitError:\n",
    "                    print(\"Rate limit exceeded. Sleeping for 30 seconds...\")\n",
    "                    time.sleep(30)\n",
    "                    retries += 1\n",
    "\n",
    "                except APIError as e:\n",
    "                    print(f\"API Error: {e}\")\n",
    "                    if \"maximum sequence length\" in str(e):\n",
    "                        print(\"Token limit exceeded. Please adjust the input size.\")\n",
    "                    retries += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Unexpected error: {e}\")\n",
    "                    break\n",
    "\n",
    "            total_samples += 1\n",
    "            time.sleep(delay)  # Respect delay between requests\n",
    "\n",
    "    accuracy = correct_predictions / total_samples if total_samples > 0 else 0\n",
    "    return accuracy, total_cost\n",
    "\n",
    "# Save results to a CSV file\n",
    "def save_results_to_csv(file_path, model_name, prompting_method, accuracy, cost):\n",
    "    # Check if the file exists\n",
    "    file_exists = os.path.isfile(file_path)\n",
    "\n",
    "    # Create a DataFrame with the current results\n",
    "    result = pd.DataFrame([{\n",
    "        'model': model_name,\n",
    "        'prompting_method': prompting_method,\n",
    "        'accuracy': accuracy,\n",
    "        'cost': cost\n",
    "    }])\n",
    "\n",
    "    # Append to the file or create a new one\n",
    "    if file_exists:\n",
    "        # Append without writing the header again\n",
    "        result.to_csv(file_path, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        # Write with header if the file is new\n",
    "        result.to_csv(file_path, index=False)\n",
    "\n",
    "# Define your model and method here\n",
    "model_name = \"Meta-Llama-3.2-1B-Instruct\"  # Change to the desired model\n",
    "prompting_method = \"chain-of-thought\"  # Change to 'direct', 'chain-of-thought', or 'few-shot'\n",
    "\n",
    "# Load dataset and evaluate\n",
    "data, locations = load_nyt_dataset()\n",
    "accuracy, cost = evaluate_model(data, locations, model_name, prompting_method)\n",
    "\n",
    "# Print and save results\n",
    "print(f\"Model: {model_name}, Prompting Method: {prompting_method}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}, Cost: ${cost:.2f}\")\n",
    "\n",
    "results_file = 'nyt_location_result'\n",
    "save_results_to_csv(results_file, model_name, prompting_method, accuracy, cost)\n",
    "print(f\"Results saved to {results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4212 tokens long, so generating 1 tokens requires a sequence length of 4213, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4212 tokens long, so generating 1 tokens requires a sequence length of 4213, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4212 tokens long, so generating 1 tokens requires a sequence length of 4213, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4212 tokens long, so generating 1 tokens requires a sequence length of 4213, but the maximum supported sequence length is just 4096!\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Skipping sample due to token limit: 6178 tokens\n",
      "Skipping sample due to token limit: 4074 tokens\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4461 tokens long, so generating 1 tokens requires a sequence length of 4462, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4461 tokens long, so generating 1 tokens requires a sequence length of 4462, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4461 tokens long, so generating 1 tokens requires a sequence length of 4462, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4461 tokens long, so generating 1 tokens requires a sequence length of 4462, but the maximum supported sequence length is just 4096!\n",
      "Skipping sample due to token limit: 4470 tokens\n",
      "Skipping sample due to token limit: 4750 tokens\n",
      "Skipping sample due to token limit: 5003 tokens\n",
      "Skipping sample due to token limit: 4547 tokens\n",
      "Skipping sample due to token limit: 4095 tokens\n",
      "Skipping sample due to token limit: 5198 tokens\n",
      "Skipping sample due to token limit: 4996 tokens\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4284 tokens long, so generating 1 tokens requires a sequence length of 4285, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4284 tokens long, so generating 1 tokens requires a sequence length of 4285, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4284 tokens long, so generating 1 tokens requires a sequence length of 4285, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4284 tokens long, so generating 1 tokens requires a sequence length of 4285, but the maximum supported sequence length is just 4096!\n",
      "Skipping sample due to token limit: 4429 tokens\n",
      "Skipping sample due to token limit: 4272 tokens\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Skipping sample due to token limit: 6121 tokens\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Skipping sample due to token limit: 8576 tokens\n",
      "Skipping sample due to token limit: 5114 tokens\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4546 tokens long, so generating 1 tokens requires a sequence length of 4547, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4546 tokens long, so generating 1 tokens requires a sequence length of 4547, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4546 tokens long, so generating 1 tokens requires a sequence length of 4547, but the maximum supported sequence length is just 4096!\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4131 tokens long, so generating 1 tokens requires a sequence length of 4132, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4131 tokens long, so generating 1 tokens requires a sequence length of 4132, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4131 tokens long, so generating 1 tokens requires a sequence length of 4132, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4131 tokens long, so generating 1 tokens requires a sequence length of 4132, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4099 tokens long, so generating 1 tokens requires a sequence length of 4100, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4099 tokens long, so generating 1 tokens requires a sequence length of 4100, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4099 tokens long, so generating 1 tokens requires a sequence length of 4100, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4099 tokens long, so generating 1 tokens requires a sequence length of 4100, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4495 tokens long, so generating 1 tokens requires a sequence length of 4496, but the maximum supported sequence length is just 4096!\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4495 tokens long, so generating 1 tokens requires a sequence length of 4496, but the maximum supported sequence length is just 4096!\n",
      "Skipping sample due to token limit: 4336 tokens\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4183 tokens long, so generating 1 tokens requires a sequence length of 4184, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4183 tokens long, so generating 1 tokens requires a sequence length of 4184, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4183 tokens long, so generating 1 tokens requires a sequence length of 4184, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4183 tokens long, so generating 1 tokens requires a sequence length of 4184, but the maximum supported sequence length is just 4096!\n",
      "Skipping sample due to token limit: 4791 tokens\n",
      "Skipping sample due to token limit: 5203 tokens\n",
      "Skipping sample due to token limit: 6826 tokens\n",
      "Skipping sample due to token limit: 5071 tokens\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4803 tokens long, so generating 1 tokens requires a sequence length of 4804, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4803 tokens long, so generating 1 tokens requires a sequence length of 4804, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4803 tokens long, so generating 1 tokens requires a sequence length of 4804, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4803 tokens long, so generating 1 tokens requires a sequence length of 4804, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4360 tokens long, so generating 1 tokens requires a sequence length of 4361, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4360 tokens long, so generating 1 tokens requires a sequence length of 4361, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4360 tokens long, so generating 1 tokens requires a sequence length of 4361, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4360 tokens long, so generating 1 tokens requires a sequence length of 4361, but the maximum supported sequence length is just 4096!\n",
      "Skipping sample due to token limit: 4200 tokens\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4735 tokens long, so generating 1 tokens requires a sequence length of 4736, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4735 tokens long, so generating 1 tokens requires a sequence length of 4736, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4735 tokens long, so generating 1 tokens requires a sequence length of 4736, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4735 tokens long, so generating 1 tokens requires a sequence length of 4736, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4267 tokens long, so generating 1 tokens requires a sequence length of 4268, but the maximum supported sequence length is just 4096!\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4267 tokens long, so generating 1 tokens requires a sequence length of 4268, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4267 tokens long, so generating 1 tokens requires a sequence length of 4268, but the maximum supported sequence length is just 4096!\n",
      "Skipping sample due to token limit: 4086 tokens\n",
      "Model: Meta-Llama-3.2-1B-Instruct, Prompting Method: few-shot\n",
      "Accuracy: 0.28, Cost: $0.02\n",
      "Results saved to nyt_location_result\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import openai\n",
    "from openai import RateLimitError, APIError\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.environ.get(\"SAMBANOVA_API_KEY\"),\n",
    "    base_url=\"https://api.sambanova.ai/v1\",\n",
    ")\n",
    "\n",
    "# Token counting using a simple string split method\n",
    "def count_tokens(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# Truncate text to fit within a specified token budget\n",
    "def truncate_text(text, max_tokens, reserve_tokens=50):\n",
    "    tokens = text.split()\n",
    "    max_input_tokens = max_tokens - reserve_tokens\n",
    "    if len(tokens) > max_input_tokens:\n",
    "        tokens = tokens[:max_input_tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Load the NYT dataset\n",
    "def load_nyt_dataset(data_dir='data/nyt_data'):\n",
    "    # File paths\n",
    "    phrase_file = os.path.join(data_dir, 'phrase_text.txt')\n",
    "    locations_file = os.path.join(data_dir, 'locations.txt')\n",
    "    locations_label_file = os.path.join(data_dir, 'locations_label.txt')\n",
    "\n",
    "    # Load phrases\n",
    "    with open(phrase_file, 'r', encoding='utf-8') as f:\n",
    "        phrases = [line.strip() for line in f]\n",
    "\n",
    "    # Load locations and their labels\n",
    "    with open(locations_file, 'r', encoding='utf-8') as f:\n",
    "        locations = [line.strip() for line in f]\n",
    "    with open(locations_label_file, 'r', encoding='utf-8') as f:\n",
    "        location_labels = [int(line.strip()) for line in f]\n",
    "\n",
    "    # Map location labels to location names\n",
    "    location_mapping = {i: name for i, name in enumerate(locations)}\n",
    "\n",
    "    # Combine data into a structured dictionary\n",
    "    data = []\n",
    "    for i, phrase in enumerate(phrases):\n",
    "        location = location_mapping.get(location_labels[i], \"Unknown\")\n",
    "        data.append({\n",
    "            'text': phrase,\n",
    "            'location': location,\n",
    "        })\n",
    "\n",
    "    return data, locations\n",
    "\n",
    "# Select random samples for evaluation\n",
    "def select_samples(data, num_samples_per_class=10):\n",
    "    classes = set(item['location'] for item in data)\n",
    "    selected_data = {cls: [] for cls in classes}\n",
    "\n",
    "    for item in data:\n",
    "        if len(selected_data[item['location']]) < num_samples_per_class:\n",
    "            selected_data[item['location']].append(item)\n",
    "\n",
    "    return selected_data\n",
    "\n",
    "# Prompting methods\n",
    "def direct_prompt(text, classes_formatted):\n",
    "    return (\n",
    "        f\"{text}\\n\\n\"\n",
    "        f\"Classify the above text into one of the following locations: {classes_formatted}.\\n\"\n",
    "        f\"Provide only the location name as your answer.\"\n",
    "    )\n",
    "\n",
    "def chain_of_thought_prompt(text, classes_formatted):\n",
    "    return (\n",
    "        f\"{text}\\n\\n\"\n",
    "        f\"Think step by step to classify the above text into one of the following locations: {classes_formatted}.\\n\"\n",
    "        f\"Provide only the location name as your answer.\"\n",
    "    )\n",
    "\n",
    "def few_shot_prompt(text, examples, classes_formatted):\n",
    "    examples_text = \"\"\n",
    "    for ex_text, ex_label in examples:\n",
    "        examples_text += f\"Text: {ex_text}\\nLabel: {ex_label}\\n\\n\"\n",
    "    return (\n",
    "        f\"{examples_text}\"\n",
    "        f\"Now, classify the following text:\\n\"\n",
    "        f\"Text: {text}\\n\\n\"\n",
    "        f\"Choose the location from the following options: {classes_formatted}.\\n\"\n",
    "        f\"Provide only the location name as your answer.\"\n",
    "    )\n",
    "\n",
    "# Cost estimation with actual prices\n",
    "def estimate_cost(input_tokens, output_tokens, model_name):\n",
    "    cost_per_million = {\n",
    "        'Llama 3.1 8B': (0.10, 0.20),\n",
    "        'Llama 3.1 70B': (0.60, 1.20),\n",
    "        'Llama 3.1 405B': (5.00, 10.00),\n",
    "        'Llama 3.2 1B': (0.04, 0.08),\n",
    "        'Llama 3.2 3B': (0.08, 0.16),\n",
    "        'Llama 3.2 11B Vision': (0.15, 0.30),\n",
    "        'Llama 3.2 90B Vision': (0.80, 1.60),\n",
    "    }\n",
    "    input_price, output_price = cost_per_million.get(model_name, (0.10, 0.20))\n",
    "    total_input_cost = (input_tokens / 1e6) * input_price\n",
    "    total_output_cost = (output_tokens / 1e6) * output_price\n",
    "    return total_input_cost + total_output_cost\n",
    "\n",
    "# Evaluate a single method and model\n",
    "def evaluate_model(\n",
    "    data, locations, model_name, prompting_method,\n",
    "    num_samples_per_class=10, token_budget=4000, delay=2, max_retries=3\n",
    "):\n",
    "    correct_predictions = 0\n",
    "    total_cost = 0\n",
    "    total_samples = 0\n",
    "    classes_formatted = \", \".join(locations)\n",
    "\n",
    "    selected_data = select_samples(data, num_samples_per_class)\n",
    "\n",
    "    for cls, samples in selected_data.items():\n",
    "        for item in samples:\n",
    "            # Truncate text to fit within token budget\n",
    "            text = truncate_text(item['text'], token_budget, reserve_tokens=100)\n",
    "\n",
    "            # Create prompt based on the chosen prompting method\n",
    "            if prompting_method == 'direct':\n",
    "                prompt = direct_prompt(text, classes_formatted)\n",
    "            elif prompting_method == 'chain-of-thought':\n",
    "                prompt = chain_of_thought_prompt(text, classes_formatted)\n",
    "            elif prompting_method == 'few-shot':\n",
    "                examples = random.sample(data, min(3, len(data)))\n",
    "                examples = [(ex['text'], ex['location']) for ex in examples]\n",
    "                prompt = few_shot_prompt(text, examples, classes_formatted)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown prompting method: {prompting_method}\")\n",
    "\n",
    "            input_token_count = count_tokens(prompt)\n",
    "            if input_token_count > token_budget:\n",
    "                print(f\"Skipping sample due to token limit: {input_token_count} tokens\")\n",
    "                continue\n",
    "\n",
    "            retries = 0\n",
    "            while retries <= max_retries:\n",
    "                try:\n",
    "                    # API call\n",
    "                    completion = client.chat.completions.create(\n",
    "                        model=model_name,\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                        stream=True,\n",
    "                        temperature=0.1,\n",
    "                        top_p=0.1\n",
    "                    )\n",
    "                    response_text = \"\".join(\n",
    "                        chunk.choices[0].delta.content or \"\" for chunk in completion\n",
    "                    )\n",
    "\n",
    "                    output_token_count = count_tokens(response_text)\n",
    "                    total_cost += estimate_cost(input_token_count, output_token_count, model_name)\n",
    "\n",
    "                    if response_text.strip().lower() == cls.lower():\n",
    "                        correct_predictions += 1\n",
    "                    break  # Exit retry loop on success\n",
    "\n",
    "                except RateLimitError:\n",
    "                    print(\"Rate limit exceeded. Sleeping for 30 seconds...\")\n",
    "                    time.sleep(30)\n",
    "                    retries += 1\n",
    "\n",
    "                except APIError as e:\n",
    "                    print(f\"API Error: {e}\")\n",
    "                    if \"maximum sequence length\" in str(e):\n",
    "                        print(\"Token limit exceeded. Please adjust the input size.\")\n",
    "                    retries += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Unexpected error: {e}\")\n",
    "                    break\n",
    "\n",
    "            total_samples += 1\n",
    "            time.sleep(delay)  # Respect delay between requests\n",
    "\n",
    "    accuracy = correct_predictions / total_samples if total_samples > 0 else 0\n",
    "    return accuracy, total_cost\n",
    "\n",
    "# Save results to a CSV file\n",
    "def save_results_to_csv(file_path, model_name, prompting_method, accuracy, cost):\n",
    "    # Check if the file exists\n",
    "    file_exists = os.path.isfile(file_path)\n",
    "\n",
    "    # Create a DataFrame with the current results\n",
    "    result = pd.DataFrame([{\n",
    "        'model': model_name,\n",
    "        'prompting_method': prompting_method,\n",
    "        'accuracy': accuracy,\n",
    "        'cost': cost\n",
    "    }])\n",
    "\n",
    "    # Append to the file or create a new one\n",
    "    if file_exists:\n",
    "        # Append without writing the header again\n",
    "        result.to_csv(file_path, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        # Write with header if the file is new\n",
    "        result.to_csv(file_path, index=False)\n",
    "\n",
    "# Define your model and method here\n",
    "model_name = \"Meta-Llama-3.2-1B-Instruct\"  # Change to the desired model\n",
    "prompting_method = \"few-shot\"  # Change to 'direct', 'chain-of-thought', or 'few-shot'\n",
    "\n",
    "# Load dataset and evaluate\n",
    "data, locations = load_nyt_dataset()\n",
    "accuracy, cost = evaluate_model(data, locations, model_name, prompting_method)\n",
    "\n",
    "# Print and save results\n",
    "print(f\"Model: {model_name}, Prompting Method: {prompting_method}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}, Cost: ${cost:.2f}\")\n",
    "\n",
    "results_file = 'nyt_location_result'\n",
    "save_results_to_csv(results_file, model_name, prompting_method, accuracy, cost)\n",
    "print(f\"Results saved to {results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4445 tokens long, so generating 1 tokens requires a sequence length of 4446, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4445 tokens long, so generating 1 tokens requires a sequence length of 4446, but the maximum supported sequence length is just 4096!\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4445 tokens long, so generating 1 tokens requires a sequence length of 4446, but the maximum supported sequence length is just 4096!\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Model: Meta-Llama-3.2-3B-Instruct, Prompting Method: direct\n",
      "Accuracy: 0.81, Cost: $0.01\n",
      "Results saved to nyt_location_result\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import openai\n",
    "from openai import RateLimitError, APIError\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.environ.get(\"SAMBANOVA_API_KEY\"),\n",
    "    base_url=\"https://api.sambanova.ai/v1\",\n",
    ")\n",
    "\n",
    "# Token counting using a simple string split method\n",
    "def count_tokens(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# Truncate text to fit within a specified token budget\n",
    "def truncate_text(text, max_tokens, reserve_tokens=50):\n",
    "    tokens = text.split()\n",
    "    max_input_tokens = max_tokens - reserve_tokens\n",
    "    if len(tokens) > max_input_tokens:\n",
    "        tokens = tokens[:max_input_tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Load the NYT dataset\n",
    "def load_nyt_dataset(data_dir='data/nyt_data'):\n",
    "    # File paths\n",
    "    phrase_file = os.path.join(data_dir, 'phrase_text.txt')\n",
    "    locations_file = os.path.join(data_dir, 'locations.txt')\n",
    "    locations_label_file = os.path.join(data_dir, 'locations_label.txt')\n",
    "\n",
    "    # Load phrases\n",
    "    with open(phrase_file, 'r', encoding='utf-8') as f:\n",
    "        phrases = [line.strip() for line in f]\n",
    "\n",
    "    # Load locations and their labels\n",
    "    with open(locations_file, 'r', encoding='utf-8') as f:\n",
    "        locations = [line.strip() for line in f]\n",
    "    with open(locations_label_file, 'r', encoding='utf-8') as f:\n",
    "        location_labels = [int(line.strip()) for line in f]\n",
    "\n",
    "    # Map location labels to location names\n",
    "    location_mapping = {i: name for i, name in enumerate(locations)}\n",
    "\n",
    "    # Combine data into a structured dictionary\n",
    "    data = []\n",
    "    for i, phrase in enumerate(phrases):\n",
    "        location = location_mapping.get(location_labels[i], \"Unknown\")\n",
    "        data.append({\n",
    "            'text': phrase,\n",
    "            'location': location,\n",
    "        })\n",
    "\n",
    "    return data, locations\n",
    "\n",
    "# Select random samples for evaluation\n",
    "def select_samples(data, num_samples_per_class=10):\n",
    "    classes = set(item['location'] for item in data)\n",
    "    selected_data = {cls: [] for cls in classes}\n",
    "\n",
    "    for item in data:\n",
    "        if len(selected_data[item['location']]) < num_samples_per_class:\n",
    "            selected_data[item['location']].append(item)\n",
    "\n",
    "    return selected_data\n",
    "\n",
    "# Prompting methods\n",
    "def direct_prompt(text, classes_formatted):\n",
    "    return (\n",
    "        f\"{text}\\n\\n\"\n",
    "        f\"Classify the above text into one of the following locations: {classes_formatted}.\\n\"\n",
    "        f\"Provide only the location name as your answer.\"\n",
    "    )\n",
    "\n",
    "def chain_of_thought_prompt(text, classes_formatted):\n",
    "    return (\n",
    "        f\"{text}\\n\\n\"\n",
    "        f\"Think step by step to classify the above text into one of the following locations: {classes_formatted}.\\n\"\n",
    "        f\"Provide only the location name as your answer.\"\n",
    "    )\n",
    "\n",
    "def few_shot_prompt(text, examples, classes_formatted):\n",
    "    examples_text = \"\"\n",
    "    for ex_text, ex_label in examples:\n",
    "        examples_text += f\"Text: {ex_text}\\nLabel: {ex_label}\\n\\n\"\n",
    "    return (\n",
    "        f\"{examples_text}\"\n",
    "        f\"Now, classify the following text:\\n\"\n",
    "        f\"Text: {text}\\n\\n\"\n",
    "        f\"Choose the location from the following options: {classes_formatted}.\\n\"\n",
    "        f\"Provide only the location name as your answer.\"\n",
    "    )\n",
    "\n",
    "# Cost estimation with actual prices\n",
    "def estimate_cost(input_tokens, output_tokens, model_name):\n",
    "    cost_per_million = {\n",
    "        'Llama 3.1 8B': (0.10, 0.20),\n",
    "        'Llama 3.1 70B': (0.60, 1.20),\n",
    "        'Llama 3.1 405B': (5.00, 10.00),\n",
    "        'Llama 3.2 1B': (0.04, 0.08),\n",
    "        'Llama 3.2 3B': (0.08, 0.16),\n",
    "        'Llama 3.2 11B Vision': (0.15, 0.30),\n",
    "        'Llama 3.2 90B Vision': (0.80, 1.60),\n",
    "    }\n",
    "    input_price, output_price = cost_per_million.get(model_name, (0.10, 0.20))\n",
    "    total_input_cost = (input_tokens / 1e6) * input_price\n",
    "    total_output_cost = (output_tokens / 1e6) * output_price\n",
    "    return total_input_cost + total_output_cost\n",
    "\n",
    "# Evaluate a single method and model\n",
    "def evaluate_model(\n",
    "    data, locations, model_name, prompting_method,\n",
    "    num_samples_per_class=10, token_budget=4000, delay=2, max_retries=3\n",
    "):\n",
    "    correct_predictions = 0\n",
    "    total_cost = 0\n",
    "    total_samples = 0\n",
    "    classes_formatted = \", \".join(locations)\n",
    "\n",
    "    selected_data = select_samples(data, num_samples_per_class)\n",
    "\n",
    "    for cls, samples in selected_data.items():\n",
    "        for item in samples:\n",
    "            # Truncate text to fit within token budget\n",
    "            text = truncate_text(item['text'], token_budget, reserve_tokens=100)\n",
    "\n",
    "            # Create prompt based on the chosen prompting method\n",
    "            if prompting_method == 'direct':\n",
    "                prompt = direct_prompt(text, classes_formatted)\n",
    "            elif prompting_method == 'chain-of-thought':\n",
    "                prompt = chain_of_thought_prompt(text, classes_formatted)\n",
    "            elif prompting_method == 'few-shot':\n",
    "                examples = random.sample(data, min(3, len(data)))\n",
    "                examples = [(ex['text'], ex['location']) for ex in examples]\n",
    "                prompt = few_shot_prompt(text, examples, classes_formatted)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown prompting method: {prompting_method}\")\n",
    "\n",
    "            input_token_count = count_tokens(prompt)\n",
    "            if input_token_count > token_budget:\n",
    "                print(f\"Skipping sample due to token limit: {input_token_count} tokens\")\n",
    "                continue\n",
    "\n",
    "            retries = 0\n",
    "            while retries <= max_retries:\n",
    "                try:\n",
    "                    # API call\n",
    "                    completion = client.chat.completions.create(\n",
    "                        model=model_name,\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                        stream=True,\n",
    "                        temperature=0.1,\n",
    "                        top_p=0.1\n",
    "                    )\n",
    "                    response_text = \"\".join(\n",
    "                        chunk.choices[0].delta.content or \"\" for chunk in completion\n",
    "                    )\n",
    "\n",
    "                    output_token_count = count_tokens(response_text)\n",
    "                    total_cost += estimate_cost(input_token_count, output_token_count, model_name)\n",
    "\n",
    "                    if response_text.strip().lower() == cls.lower():\n",
    "                        correct_predictions += 1\n",
    "                    break  # Exit retry loop on success\n",
    "\n",
    "                except RateLimitError:\n",
    "                    print(\"Rate limit exceeded. Sleeping for 30 seconds...\")\n",
    "                    time.sleep(30)\n",
    "                    retries += 1\n",
    "\n",
    "                except APIError as e:\n",
    "                    print(f\"API Error: {e}\")\n",
    "                    if \"maximum sequence length\" in str(e):\n",
    "                        print(\"Token limit exceeded. Please adjust the input size.\")\n",
    "                    retries += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Unexpected error: {e}\")\n",
    "                    break\n",
    "\n",
    "            total_samples += 1\n",
    "            time.sleep(delay)  # Respect delay between requests\n",
    "\n",
    "    accuracy = correct_predictions / total_samples if total_samples > 0 else 0\n",
    "    return accuracy, total_cost\n",
    "\n",
    "# Save results to a CSV file\n",
    "def save_results_to_csv(file_path, model_name, prompting_method, accuracy, cost):\n",
    "    # Check if the file exists\n",
    "    file_exists = os.path.isfile(file_path)\n",
    "\n",
    "    # Create a DataFrame with the current results\n",
    "    result = pd.DataFrame([{\n",
    "        'model': model_name,\n",
    "        'prompting_method': prompting_method,\n",
    "        'accuracy': accuracy,\n",
    "        'cost': cost\n",
    "    }])\n",
    "\n",
    "    # Append to the file or create a new one\n",
    "    if file_exists:\n",
    "        # Append without writing the header again\n",
    "        result.to_csv(file_path, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        # Write with header if the file is new\n",
    "        result.to_csv(file_path, index=False)\n",
    "\n",
    "# Define your model and method here\n",
    "model_name = \"Meta-Llama-3.2-3B-Instruct\"  # Change to the desired model\n",
    "prompting_method = \"direct\"  # Change to 'direct', 'chain-of-thought', or 'few-shot'\n",
    "\n",
    "# Load dataset and evaluate\n",
    "data, locations = load_nyt_dataset()\n",
    "accuracy, cost = evaluate_model(data, locations, model_name, prompting_method)\n",
    "\n",
    "# Print and save results\n",
    "print(f\"Model: {model_name}, Prompting Method: {prompting_method}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}, Cost: ${cost:.2f}\")\n",
    "\n",
    "results_file = 'nyt_location_result'\n",
    "save_results_to_csv(results_file, model_name, prompting_method, accuracy, cost)\n",
    "print(f\"Results saved to {results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4449 tokens long, so generating 1 tokens requires a sequence length of 4450, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4449 tokens long, so generating 1 tokens requires a sequence length of 4450, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4449 tokens long, so generating 1 tokens requires a sequence length of 4450, but the maximum supported sequence length is just 4096!\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Model: Meta-Llama-3.2-3B-Instruct, Prompting Method: chain-of-thought\n",
      "Accuracy: 0.78, Cost: $0.01\n",
      "Results saved to nyt_location_result\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import openai\n",
    "from openai import RateLimitError, APIError\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.environ.get(\"SAMBANOVA_API_KEY\"),\n",
    "    base_url=\"https://api.sambanova.ai/v1\",\n",
    ")\n",
    "\n",
    "# Token counting using a simple string split method\n",
    "def count_tokens(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# Truncate text to fit within a specified token budget\n",
    "def truncate_text(text, max_tokens, reserve_tokens=50):\n",
    "    tokens = text.split()\n",
    "    max_input_tokens = max_tokens - reserve_tokens\n",
    "    if len(tokens) > max_input_tokens:\n",
    "        tokens = tokens[:max_input_tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Load the NYT dataset\n",
    "def load_nyt_dataset(data_dir='data/nyt_data'):\n",
    "    # File paths\n",
    "    phrase_file = os.path.join(data_dir, 'phrase_text.txt')\n",
    "    locations_file = os.path.join(data_dir, 'locations.txt')\n",
    "    locations_label_file = os.path.join(data_dir, 'locations_label.txt')\n",
    "\n",
    "    # Load phrases\n",
    "    with open(phrase_file, 'r', encoding='utf-8') as f:\n",
    "        phrases = [line.strip() for line in f]\n",
    "\n",
    "    # Load locations and their labels\n",
    "    with open(locations_file, 'r', encoding='utf-8') as f:\n",
    "        locations = [line.strip() for line in f]\n",
    "    with open(locations_label_file, 'r', encoding='utf-8') as f:\n",
    "        location_labels = [int(line.strip()) for line in f]\n",
    "\n",
    "    # Map location labels to location names\n",
    "    location_mapping = {i: name for i, name in enumerate(locations)}\n",
    "\n",
    "    # Combine data into a structured dictionary\n",
    "    data = []\n",
    "    for i, phrase in enumerate(phrases):\n",
    "        location = location_mapping.get(location_labels[i], \"Unknown\")\n",
    "        data.append({\n",
    "            'text': phrase,\n",
    "            'location': location,\n",
    "        })\n",
    "\n",
    "    return data, locations\n",
    "\n",
    "# Select random samples for evaluation\n",
    "def select_samples(data, num_samples_per_class=10):\n",
    "    classes = set(item['location'] for item in data)\n",
    "    selected_data = {cls: [] for cls in classes}\n",
    "\n",
    "    for item in data:\n",
    "        if len(selected_data[item['location']]) < num_samples_per_class:\n",
    "            selected_data[item['location']].append(item)\n",
    "\n",
    "    return selected_data\n",
    "\n",
    "# Prompting methods\n",
    "def direct_prompt(text, classes_formatted):\n",
    "    return (\n",
    "        f\"{text}\\n\\n\"\n",
    "        f\"Classify the above text into one of the following locations: {classes_formatted}.\\n\"\n",
    "        f\"Provide only the location name as your answer.\"\n",
    "    )\n",
    "\n",
    "def chain_of_thought_prompt(text, classes_formatted):\n",
    "    return (\n",
    "        f\"{text}\\n\\n\"\n",
    "        f\"Think step by step to classify the above text into one of the following locations: {classes_formatted}.\\n\"\n",
    "        f\"Provide only the location name as your answer.\"\n",
    "    )\n",
    "\n",
    "def few_shot_prompt(text, examples, classes_formatted):\n",
    "    examples_text = \"\"\n",
    "    for ex_text, ex_label in examples:\n",
    "        examples_text += f\"Text: {ex_text}\\nLabel: {ex_label}\\n\\n\"\n",
    "    return (\n",
    "        f\"{examples_text}\"\n",
    "        f\"Now, classify the following text:\\n\"\n",
    "        f\"Text: {text}\\n\\n\"\n",
    "        f\"Choose the location from the following options: {classes_formatted}.\\n\"\n",
    "        f\"Provide only the location name as your answer.\"\n",
    "    )\n",
    "\n",
    "# Cost estimation with actual prices\n",
    "def estimate_cost(input_tokens, output_tokens, model_name):\n",
    "    cost_per_million = {\n",
    "        'Llama 3.1 8B': (0.10, 0.20),\n",
    "        'Llama 3.1 70B': (0.60, 1.20),\n",
    "        'Llama 3.1 405B': (5.00, 10.00),\n",
    "        'Llama 3.2 1B': (0.04, 0.08),\n",
    "        'Llama 3.2 3B': (0.08, 0.16),\n",
    "        'Llama 3.2 11B Vision': (0.15, 0.30),\n",
    "        'Llama 3.2 90B Vision': (0.80, 1.60),\n",
    "    }\n",
    "    input_price, output_price = cost_per_million.get(model_name, (0.10, 0.20))\n",
    "    total_input_cost = (input_tokens / 1e6) * input_price\n",
    "    total_output_cost = (output_tokens / 1e6) * output_price\n",
    "    return total_input_cost + total_output_cost\n",
    "\n",
    "# Evaluate a single method and model\n",
    "def evaluate_model(\n",
    "    data, locations, model_name, prompting_method,\n",
    "    num_samples_per_class=10, token_budget=4000, delay=2, max_retries=3\n",
    "):\n",
    "    correct_predictions = 0\n",
    "    total_cost = 0\n",
    "    total_samples = 0\n",
    "    classes_formatted = \", \".join(locations)\n",
    "\n",
    "    selected_data = select_samples(data, num_samples_per_class)\n",
    "\n",
    "    for cls, samples in selected_data.items():\n",
    "        for item in samples:\n",
    "            # Truncate text to fit within token budget\n",
    "            text = truncate_text(item['text'], token_budget, reserve_tokens=100)\n",
    "\n",
    "            # Create prompt based on the chosen prompting method\n",
    "            if prompting_method == 'direct':\n",
    "                prompt = direct_prompt(text, classes_formatted)\n",
    "            elif prompting_method == 'chain-of-thought':\n",
    "                prompt = chain_of_thought_prompt(text, classes_formatted)\n",
    "            elif prompting_method == 'few-shot':\n",
    "                examples = random.sample(data, min(3, len(data)))\n",
    "                examples = [(ex['text'], ex['location']) for ex in examples]\n",
    "                prompt = few_shot_prompt(text, examples, classes_formatted)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown prompting method: {prompting_method}\")\n",
    "\n",
    "            input_token_count = count_tokens(prompt)\n",
    "            if input_token_count > token_budget:\n",
    "                print(f\"Skipping sample due to token limit: {input_token_count} tokens\")\n",
    "                continue\n",
    "\n",
    "            retries = 0\n",
    "            while retries <= max_retries:\n",
    "                try:\n",
    "                    # API call\n",
    "                    completion = client.chat.completions.create(\n",
    "                        model=model_name,\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                        stream=True,\n",
    "                        temperature=0.1,\n",
    "                        top_p=0.1\n",
    "                    )\n",
    "                    response_text = \"\".join(\n",
    "                        chunk.choices[0].delta.content or \"\" for chunk in completion\n",
    "                    )\n",
    "\n",
    "                    output_token_count = count_tokens(response_text)\n",
    "                    total_cost += estimate_cost(input_token_count, output_token_count, model_name)\n",
    "\n",
    "                    if response_text.strip().lower() == cls.lower():\n",
    "                        correct_predictions += 1\n",
    "                    break  # Exit retry loop on success\n",
    "\n",
    "                except RateLimitError:\n",
    "                    print(\"Rate limit exceeded. Sleeping for 30 seconds...\")\n",
    "                    time.sleep(30)\n",
    "                    retries += 1\n",
    "\n",
    "                except APIError as e:\n",
    "                    print(f\"API Error: {e}\")\n",
    "                    if \"maximum sequence length\" in str(e):\n",
    "                        print(\"Token limit exceeded. Please adjust the input size.\")\n",
    "                    retries += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Unexpected error: {e}\")\n",
    "                    break\n",
    "\n",
    "            total_samples += 1\n",
    "            time.sleep(delay)  # Respect delay between requests\n",
    "\n",
    "    accuracy = correct_predictions / total_samples if total_samples > 0 else 0\n",
    "    return accuracy, total_cost\n",
    "\n",
    "# Save results to a CSV file\n",
    "def save_results_to_csv(file_path, model_name, prompting_method, accuracy, cost):\n",
    "    # Check if the file exists\n",
    "    file_exists = os.path.isfile(file_path)\n",
    "\n",
    "    # Create a DataFrame with the current results\n",
    "    result = pd.DataFrame([{\n",
    "        'model': model_name,\n",
    "        'prompting_method': prompting_method,\n",
    "        'accuracy': accuracy,\n",
    "        'cost': cost\n",
    "    }])\n",
    "\n",
    "    # Append to the file or create a new one\n",
    "    if file_exists:\n",
    "        # Append without writing the header again\n",
    "        result.to_csv(file_path, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        # Write with header if the file is new\n",
    "        result.to_csv(file_path, index=False)\n",
    "\n",
    "# Define your model and method here\n",
    "model_name = \"Meta-Llama-3.2-3B-Instruct\"  # Change to the desired model\n",
    "prompting_method = \"chain-of-thought\"  # Change to 'direct', 'chain-of-thought', or 'few-shot'\n",
    "\n",
    "# Load dataset and evaluate\n",
    "data, locations = load_nyt_dataset()\n",
    "accuracy, cost = evaluate_model(data, locations, model_name, prompting_method)\n",
    "\n",
    "# Print and save results\n",
    "print(f\"Model: {model_name}, Prompting Method: {prompting_method}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}, Cost: ${cost:.2f}\")\n",
    "\n",
    "results_file = 'nyt_location_result'\n",
    "save_results_to_csv(results_file, model_name, prompting_method, accuracy, cost)\n",
    "print(f\"Results saved to {results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4213 tokens long, so generating 1 tokens requires a sequence length of 4214, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4213 tokens long, so generating 1 tokens requires a sequence length of 4214, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4213 tokens long, so generating 1 tokens requires a sequence length of 4214, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4213 tokens long, so generating 1 tokens requires a sequence length of 4214, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4310 tokens long, so generating 1 tokens requires a sequence length of 4311, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4310 tokens long, so generating 1 tokens requires a sequence length of 4311, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4310 tokens long, so generating 1 tokens requires a sequence length of 4311, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4310 tokens long, so generating 1 tokens requires a sequence length of 4311, but the maximum supported sequence length is just 4096!\n",
      "Skipping sample due to token limit: 6770 tokens\n",
      "Skipping sample due to token limit: 4770 tokens\n",
      "Skipping sample due to token limit: 5369 tokens\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4710 tokens long, so generating 1 tokens requires a sequence length of 4711, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4710 tokens long, so generating 1 tokens requires a sequence length of 4711, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4710 tokens long, so generating 1 tokens requires a sequence length of 4711, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4710 tokens long, so generating 1 tokens requires a sequence length of 4711, but the maximum supported sequence length is just 4096!\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Skipping sample due to token limit: 4904 tokens\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4573 tokens long, so generating 1 tokens requires a sequence length of 4574, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4573 tokens long, so generating 1 tokens requires a sequence length of 4574, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4573 tokens long, so generating 1 tokens requires a sequence length of 4574, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4573 tokens long, so generating 1 tokens requires a sequence length of 4574, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4488 tokens long, so generating 1 tokens requires a sequence length of 4489, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4488 tokens long, so generating 1 tokens requires a sequence length of 4489, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4488 tokens long, so generating 1 tokens requires a sequence length of 4489, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4488 tokens long, so generating 1 tokens requires a sequence length of 4489, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4384 tokens long, so generating 1 tokens requires a sequence length of 4385, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4384 tokens long, so generating 1 tokens requires a sequence length of 4385, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4384 tokens long, so generating 1 tokens requires a sequence length of 4385, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4384 tokens long, so generating 1 tokens requires a sequence length of 4385, but the maximum supported sequence length is just 4096!\n",
      "Skipping sample due to token limit: 4464 tokens\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4783 tokens long, so generating 1 tokens requires a sequence length of 4784, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4783 tokens long, so generating 1 tokens requires a sequence length of 4784, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4783 tokens long, so generating 1 tokens requires a sequence length of 4784, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4783 tokens long, so generating 1 tokens requires a sequence length of 4784, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4885 tokens long, so generating 1 tokens requires a sequence length of 4886, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4885 tokens long, so generating 1 tokens requires a sequence length of 4886, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4885 tokens long, so generating 1 tokens requires a sequence length of 4886, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4885 tokens long, so generating 1 tokens requires a sequence length of 4886, but the maximum supported sequence length is just 4096!\n",
      "Skipping sample due to token limit: 4099 tokens\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Skipping sample due to token limit: 4120 tokens\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4258 tokens long, so generating 1 tokens requires a sequence length of 4259, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4258 tokens long, so generating 1 tokens requires a sequence length of 4259, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4258 tokens long, so generating 1 tokens requires a sequence length of 4259, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4258 tokens long, so generating 1 tokens requires a sequence length of 4259, but the maximum supported sequence length is just 4096!\n",
      "Skipping sample due to token limit: 4717 tokens\n",
      "Skipping sample due to token limit: 4790 tokens\n",
      "Skipping sample due to token limit: 4810 tokens\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4756 tokens long, so generating 1 tokens requires a sequence length of 4757, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4756 tokens long, so generating 1 tokens requires a sequence length of 4757, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4756 tokens long, so generating 1 tokens requires a sequence length of 4757, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4756 tokens long, so generating 1 tokens requires a sequence length of 4757, but the maximum supported sequence length is just 4096!\n",
      "Skipping sample due to token limit: 4292 tokens\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4559 tokens long, so generating 1 tokens requires a sequence length of 4560, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4559 tokens long, so generating 1 tokens requires a sequence length of 4560, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4559 tokens long, so generating 1 tokens requires a sequence length of 4560, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4559 tokens long, so generating 1 tokens requires a sequence length of 4560, but the maximum supported sequence length is just 4096!\n",
      "Skipping sample due to token limit: 5123 tokens\n",
      "Skipping sample due to token limit: 4706 tokens\n",
      "Skipping sample due to token limit: 4793 tokens\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4310 tokens long, so generating 1 tokens requires a sequence length of 4311, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4310 tokens long, so generating 1 tokens requires a sequence length of 4311, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4310 tokens long, so generating 1 tokens requires a sequence length of 4311, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4310 tokens long, so generating 1 tokens requires a sequence length of 4311, but the maximum supported sequence length is just 4096!\n",
      "Skipping sample due to token limit: 5235 tokens\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4466 tokens long, so generating 1 tokens requires a sequence length of 4467, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4466 tokens long, so generating 1 tokens requires a sequence length of 4467, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4466 tokens long, so generating 1 tokens requires a sequence length of 4467, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4466 tokens long, so generating 1 tokens requires a sequence length of 4467, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4656 tokens long, so generating 1 tokens requires a sequence length of 4657, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4656 tokens long, so generating 1 tokens requires a sequence length of 4657, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4656 tokens long, so generating 1 tokens requires a sequence length of 4657, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4656 tokens long, so generating 1 tokens requires a sequence length of 4657, but the maximum supported sequence length is just 4096!\n",
      "Skipping sample due to token limit: 4615 tokens\n",
      "Skipping sample due to token limit: 4995 tokens\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4264 tokens long, so generating 1 tokens requires a sequence length of 4265, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4264 tokens long, so generating 1 tokens requires a sequence length of 4265, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4264 tokens long, so generating 1 tokens requires a sequence length of 4265, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4264 tokens long, so generating 1 tokens requires a sequence length of 4265, but the maximum supported sequence length is just 4096!\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Skipping sample due to token limit: 4398 tokens\n",
      "Skipping sample due to token limit: 5151 tokens\n",
      "Skipping sample due to token limit: 4357 tokens\n",
      "Model: Meta-Llama-3.2-3B-Instruct, Prompting Method: few-shot\n",
      "Accuracy: 0.65, Cost: $0.01\n",
      "Results saved to nyt_location_result\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import openai\n",
    "from openai import RateLimitError, APIError\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.environ.get(\"SAMBANOVA_API_KEY\"),\n",
    "    base_url=\"https://api.sambanova.ai/v1\",\n",
    ")\n",
    "\n",
    "# Token counting using a simple string split method\n",
    "def count_tokens(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# Truncate text to fit within a specified token budget\n",
    "def truncate_text(text, max_tokens, reserve_tokens=50):\n",
    "    tokens = text.split()\n",
    "    max_input_tokens = max_tokens - reserve_tokens\n",
    "    if len(tokens) > max_input_tokens:\n",
    "        tokens = tokens[:max_input_tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Load the NYT dataset\n",
    "def load_nyt_dataset(data_dir='data/nyt_data'):\n",
    "    # File paths\n",
    "    phrase_file = os.path.join(data_dir, 'phrase_text.txt')\n",
    "    locations_file = os.path.join(data_dir, 'locations.txt')\n",
    "    locations_label_file = os.path.join(data_dir, 'locations_label.txt')\n",
    "\n",
    "    # Load phrases\n",
    "    with open(phrase_file, 'r', encoding='utf-8') as f:\n",
    "        phrases = [line.strip() for line in f]\n",
    "\n",
    "    # Load locations and their labels\n",
    "    with open(locations_file, 'r', encoding='utf-8') as f:\n",
    "        locations = [line.strip() for line in f]\n",
    "    with open(locations_label_file, 'r', encoding='utf-8') as f:\n",
    "        location_labels = [int(line.strip()) for line in f]\n",
    "\n",
    "    # Map location labels to location names\n",
    "    location_mapping = {i: name for i, name in enumerate(locations)}\n",
    "\n",
    "    # Combine data into a structured dictionary\n",
    "    data = []\n",
    "    for i, phrase in enumerate(phrases):\n",
    "        location = location_mapping.get(location_labels[i], \"Unknown\")\n",
    "        data.append({\n",
    "            'text': phrase,\n",
    "            'location': location,\n",
    "        })\n",
    "\n",
    "    return data, locations\n",
    "\n",
    "# Select random samples for evaluation\n",
    "def select_samples(data, num_samples_per_class=10):\n",
    "    classes = set(item['location'] for item in data)\n",
    "    selected_data = {cls: [] for cls in classes}\n",
    "\n",
    "    for item in data:\n",
    "        if len(selected_data[item['location']]) < num_samples_per_class:\n",
    "            selected_data[item['location']].append(item)\n",
    "\n",
    "    return selected_data\n",
    "\n",
    "# Prompting methods\n",
    "def direct_prompt(text, classes_formatted):\n",
    "    return (\n",
    "        f\"{text}\\n\\n\"\n",
    "        f\"Classify the above text into one of the following locations: {classes_formatted}.\\n\"\n",
    "        f\"Provide only the location name as your answer.\"\n",
    "    )\n",
    "\n",
    "def chain_of_thought_prompt(text, classes_formatted):\n",
    "    return (\n",
    "        f\"{text}\\n\\n\"\n",
    "        f\"Think step by step to classify the above text into one of the following locations: {classes_formatted}.\\n\"\n",
    "        f\"Provide only the location name as your answer.\"\n",
    "    )\n",
    "\n",
    "def few_shot_prompt(text, examples, classes_formatted):\n",
    "    examples_text = \"\"\n",
    "    for ex_text, ex_label in examples:\n",
    "        examples_text += f\"Text: {ex_text}\\nLabel: {ex_label}\\n\\n\"\n",
    "    return (\n",
    "        f\"{examples_text}\"\n",
    "        f\"Now, classify the following text:\\n\"\n",
    "        f\"Text: {text}\\n\\n\"\n",
    "        f\"Choose the location from the following options: {classes_formatted}.\\n\"\n",
    "        f\"Provide only the location name as your answer.\"\n",
    "    )\n",
    "\n",
    "# Cost estimation with actual prices\n",
    "def estimate_cost(input_tokens, output_tokens, model_name):\n",
    "    cost_per_million = {\n",
    "        'Llama 3.1 8B': (0.10, 0.20),\n",
    "        'Llama 3.1 70B': (0.60, 1.20),\n",
    "        'Llama 3.1 405B': (5.00, 10.00),\n",
    "        'Llama 3.2 1B': (0.04, 0.08),\n",
    "        'Llama 3.2 3B': (0.08, 0.16),\n",
    "        'Llama 3.2 11B Vision': (0.15, 0.30),\n",
    "        'Llama 3.2 90B Vision': (0.80, 1.60),\n",
    "    }\n",
    "    input_price, output_price = cost_per_million.get(model_name, (0.10, 0.20))\n",
    "    total_input_cost = (input_tokens / 1e6) * input_price\n",
    "    total_output_cost = (output_tokens / 1e6) * output_price\n",
    "    return total_input_cost + total_output_cost\n",
    "\n",
    "# Evaluate a single method and model\n",
    "def evaluate_model(\n",
    "    data, locations, model_name, prompting_method,\n",
    "    num_samples_per_class=10, token_budget=4000, delay=2, max_retries=3\n",
    "):\n",
    "    correct_predictions = 0\n",
    "    total_cost = 0\n",
    "    total_samples = 0\n",
    "    classes_formatted = \", \".join(locations)\n",
    "\n",
    "    selected_data = select_samples(data, num_samples_per_class)\n",
    "\n",
    "    for cls, samples in selected_data.items():\n",
    "        for item in samples:\n",
    "            # Truncate text to fit within token budget\n",
    "            text = truncate_text(item['text'], token_budget, reserve_tokens=100)\n",
    "\n",
    "            # Create prompt based on the chosen prompting method\n",
    "            if prompting_method == 'direct':\n",
    "                prompt = direct_prompt(text, classes_formatted)\n",
    "            elif prompting_method == 'chain-of-thought':\n",
    "                prompt = chain_of_thought_prompt(text, classes_formatted)\n",
    "            elif prompting_method == 'few-shot':\n",
    "                examples = random.sample(data, min(3, len(data)))\n",
    "                examples = [(ex['text'], ex['location']) for ex in examples]\n",
    "                prompt = few_shot_prompt(text, examples, classes_formatted)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown prompting method: {prompting_method}\")\n",
    "\n",
    "            input_token_count = count_tokens(prompt)\n",
    "            if input_token_count > token_budget:\n",
    "                print(f\"Skipping sample due to token limit: {input_token_count} tokens\")\n",
    "                continue\n",
    "\n",
    "            retries = 0\n",
    "            while retries <= max_retries:\n",
    "                try:\n",
    "                    # API call\n",
    "                    completion = client.chat.completions.create(\n",
    "                        model=model_name,\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                        stream=True,\n",
    "                        temperature=0.1,\n",
    "                        top_p=0.1\n",
    "                    )\n",
    "                    response_text = \"\".join(\n",
    "                        chunk.choices[0].delta.content or \"\" for chunk in completion\n",
    "                    )\n",
    "\n",
    "                    output_token_count = count_tokens(response_text)\n",
    "                    total_cost += estimate_cost(input_token_count, output_token_count, model_name)\n",
    "\n",
    "                    if response_text.strip().lower() == cls.lower():\n",
    "                        correct_predictions += 1\n",
    "                    break  # Exit retry loop on success\n",
    "\n",
    "                except RateLimitError:\n",
    "                    print(\"Rate limit exceeded. Sleeping for 30 seconds...\")\n",
    "                    time.sleep(30)\n",
    "                    retries += 1\n",
    "\n",
    "                except APIError as e:\n",
    "                    print(f\"API Error: {e}\")\n",
    "                    if \"maximum sequence length\" in str(e):\n",
    "                        print(\"Token limit exceeded. Please adjust the input size.\")\n",
    "                    retries += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Unexpected error: {e}\")\n",
    "                    break\n",
    "\n",
    "            total_samples += 1\n",
    "            time.sleep(delay)  # Respect delay between requests\n",
    "\n",
    "    accuracy = correct_predictions / total_samples if total_samples > 0 else 0\n",
    "    return accuracy, total_cost\n",
    "\n",
    "# Save results to a CSV file\n",
    "def save_results_to_csv(file_path, model_name, prompting_method, accuracy, cost):\n",
    "    # Check if the file exists\n",
    "    file_exists = os.path.isfile(file_path)\n",
    "\n",
    "    # Create a DataFrame with the current results\n",
    "    result = pd.DataFrame([{\n",
    "        'model': model_name,\n",
    "        'prompting_method': prompting_method,\n",
    "        'accuracy': accuracy,\n",
    "        'cost': cost\n",
    "    }])\n",
    "\n",
    "    # Append to the file or create a new one\n",
    "    if file_exists:\n",
    "        # Append without writing the header again\n",
    "        result.to_csv(file_path, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        # Write with header if the file is new\n",
    "        result.to_csv(file_path, index=False)\n",
    "\n",
    "# Define your model and method here\n",
    "model_name = \"Meta-Llama-3.2-3B-Instruct\"   # Change to the desired model\n",
    "prompting_method = \"few-shot\"  # Change to 'direct', 'chain-of-thought', or 'few-shot'\n",
    "\n",
    "# Load dataset and evaluate\n",
    "data, locations = load_nyt_dataset()\n",
    "accuracy, cost = evaluate_model(data, locations, model_name, prompting_method)\n",
    "\n",
    "# Print and save results\n",
    "print(f\"Model: {model_name}, Prompting Method: {prompting_method}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}, Cost: ${cost:.2f}\")\n",
    "\n",
    "results_file = 'nyt_location_result'\n",
    "save_results_to_csv(results_file, model_name, prompting_method, accuracy, cost)\n",
    "print(f\"Results saved to {results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Meta-Llama-3.1-8B-Instruct, Prompting Method: direct\n",
      "Accuracy: 0.87, Cost: $0.01\n",
      "Results saved to nyt_location_result\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import openai\n",
    "from openai import RateLimitError, APIError\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(\n",
    "    aapi_key=os.environ.get(\"SAMBANOVA_API_KEY\"),\n",
    "    base_url=\"https://api.sambanova.ai/v1\",\n",
    ")\n",
    "\n",
    "# Token counting using a simple string split method\n",
    "def count_tokens(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# Truncate text to fit within a specified token budget\n",
    "def truncate_text(text, max_tokens, reserve_tokens=50):\n",
    "    tokens = text.split()\n",
    "    max_input_tokens = max_tokens - reserve_tokens\n",
    "    if len(tokens) > max_input_tokens:\n",
    "        tokens = tokens[:max_input_tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Load the NYT dataset\n",
    "def load_nyt_dataset(data_dir='data/nyt_data'):\n",
    "    # File paths\n",
    "    phrase_file = os.path.join(data_dir, 'phrase_text.txt')\n",
    "    locations_file = os.path.join(data_dir, 'locations.txt')\n",
    "    locations_label_file = os.path.join(data_dir, 'locations_label.txt')\n",
    "\n",
    "    # Load phrases\n",
    "    with open(phrase_file, 'r', encoding='utf-8') as f:\n",
    "        phrases = [line.strip() for line in f]\n",
    "\n",
    "    # Load locations and their labels\n",
    "    with open(locations_file, 'r', encoding='utf-8') as f:\n",
    "        locations = [line.strip() for line in f]\n",
    "    with open(locations_label_file, 'r', encoding='utf-8') as f:\n",
    "        location_labels = [int(line.strip()) for line in f]\n",
    "\n",
    "    # Map location labels to location names\n",
    "    location_mapping = {i: name for i, name in enumerate(locations)}\n",
    "\n",
    "    # Combine data into a structured dictionary\n",
    "    data = []\n",
    "    for i, phrase in enumerate(phrases):\n",
    "        location = location_mapping.get(location_labels[i], \"Unknown\")\n",
    "        data.append({\n",
    "            'text': phrase,\n",
    "            'location': location,\n",
    "        })\n",
    "\n",
    "    return data, locations\n",
    "\n",
    "# Select random samples for evaluation\n",
    "def select_samples(data, num_samples_per_class=10):\n",
    "    classes = set(item['location'] for item in data)\n",
    "    selected_data = {cls: [] for cls in classes}\n",
    "\n",
    "    for item in data:\n",
    "        if len(selected_data[item['location']]) < num_samples_per_class:\n",
    "            selected_data[item['location']].append(item)\n",
    "\n",
    "    return selected_data\n",
    "\n",
    "# Prompting methods\n",
    "def direct_prompt(text, classes_formatted):\n",
    "    return (\n",
    "        f\"{text}\\n\\n\"\n",
    "        f\"Classify the above text into one of the following locations: {classes_formatted}.\\n\"\n",
    "        f\"Provide only the location name as your answer.\"\n",
    "    )\n",
    "\n",
    "def chain_of_thought_prompt(text, classes_formatted):\n",
    "    return (\n",
    "        f\"{text}\\n\\n\"\n",
    "        f\"Think step by step to classify the above text into one of the following locations: {classes_formatted}.\\n\"\n",
    "        f\"Provide only the location name as your answer.\"\n",
    "    )\n",
    "\n",
    "def few_shot_prompt(text, examples, classes_formatted):\n",
    "    examples_text = \"\"\n",
    "    for ex_text, ex_label in examples:\n",
    "        examples_text += f\"Text: {ex_text}\\nLabel: {ex_label}\\n\\n\"\n",
    "    return (\n",
    "        f\"{examples_text}\"\n",
    "        f\"Now, classify the following text:\\n\"\n",
    "        f\"Text: {text}\\n\\n\"\n",
    "        f\"Choose the location from the following options: {classes_formatted}.\\n\"\n",
    "        f\"Provide only the location name as your answer.\"\n",
    "    )\n",
    "\n",
    "# Cost estimation with actual prices\n",
    "def estimate_cost(input_tokens, output_tokens, model_name):\n",
    "    cost_per_million = {\n",
    "        'Llama 3.1 8B': (0.10, 0.20),\n",
    "        'Llama 3.1 70B': (0.60, 1.20),\n",
    "        'Llama 3.1 405B': (5.00, 10.00),\n",
    "        'Llama 3.2 1B': (0.04, 0.08),\n",
    "        'Llama 3.2 3B': (0.08, 0.16),\n",
    "        'Llama 3.2 11B Vision': (0.15, 0.30),\n",
    "        'Llama 3.2 90B Vision': (0.80, 1.60),\n",
    "    }\n",
    "    input_price, output_price = cost_per_million.get(model_name, (0.10, 0.20))\n",
    "    total_input_cost = (input_tokens / 1e6) * input_price\n",
    "    total_output_cost = (output_tokens / 1e6) * output_price\n",
    "    return total_input_cost + total_output_cost\n",
    "\n",
    "# Evaluate a single method and model\n",
    "def evaluate_model(\n",
    "    data, locations, model_name, prompting_method,\n",
    "    num_samples_per_class=10, token_budget=4000, delay=2, max_retries=3\n",
    "):\n",
    "    correct_predictions = 0\n",
    "    total_cost = 0\n",
    "    total_samples = 0\n",
    "    classes_formatted = \", \".join(locations)\n",
    "\n",
    "    selected_data = select_samples(data, num_samples_per_class)\n",
    "\n",
    "    for cls, samples in selected_data.items():\n",
    "        for item in samples:\n",
    "            # Truncate text to fit within token budget\n",
    "            text = truncate_text(item['text'], token_budget, reserve_tokens=100)\n",
    "\n",
    "            # Create prompt based on the chosen prompting method\n",
    "            if prompting_method == 'direct':\n",
    "                prompt = direct_prompt(text, classes_formatted)\n",
    "            elif prompting_method == 'chain-of-thought':\n",
    "                prompt = chain_of_thought_prompt(text, classes_formatted)\n",
    "            elif prompting_method == 'few-shot':\n",
    "                examples = random.sample(data, min(3, len(data)))\n",
    "                examples = [(ex['text'], ex['location']) for ex in examples]\n",
    "                prompt = few_shot_prompt(text, examples, classes_formatted)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown prompting method: {prompting_method}\")\n",
    "\n",
    "            input_token_count = count_tokens(prompt)\n",
    "            if input_token_count > token_budget:\n",
    "                print(f\"Skipping sample due to token limit: {input_token_count} tokens\")\n",
    "                continue\n",
    "\n",
    "            retries = 0\n",
    "            while retries <= max_retries:\n",
    "                try:\n",
    "                    # API call\n",
    "                    completion = client.chat.completions.create(\n",
    "                        model=model_name,\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                        stream=True,\n",
    "                        temperature=0.1,\n",
    "                        top_p=0.1\n",
    "                    )\n",
    "                    response_text = \"\".join(\n",
    "                        chunk.choices[0].delta.content or \"\" for chunk in completion\n",
    "                    )\n",
    "\n",
    "                    output_token_count = count_tokens(response_text)\n",
    "                    total_cost += estimate_cost(input_token_count, output_token_count, model_name)\n",
    "\n",
    "                    if response_text.strip().lower() == cls.lower():\n",
    "                        correct_predictions += 1\n",
    "                    break  # Exit retry loop on success\n",
    "\n",
    "                except RateLimitError:\n",
    "                    print(\"Rate limit exceeded. Sleeping for 30 seconds...\")\n",
    "                    time.sleep(30)\n",
    "                    retries += 1\n",
    "\n",
    "                except APIError as e:\n",
    "                    print(f\"API Error: {e}\")\n",
    "                    if \"maximum sequence length\" in str(e):\n",
    "                        print(\"Token limit exceeded. Please adjust the input size.\")\n",
    "                    retries += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Unexpected error: {e}\")\n",
    "                    break\n",
    "\n",
    "            total_samples += 1\n",
    "            time.sleep(delay)  # Respect delay between requests\n",
    "\n",
    "    accuracy = correct_predictions / total_samples if total_samples > 0 else 0\n",
    "    return accuracy, total_cost\n",
    "\n",
    "# Save results to a CSV file\n",
    "def save_results_to_csv(file_path, model_name, prompting_method, accuracy, cost):\n",
    "    # Check if the file exists\n",
    "    file_exists = os.path.isfile(file_path)\n",
    "\n",
    "    # Create a DataFrame with the current results\n",
    "    result = pd.DataFrame([{\n",
    "        'model': model_name,\n",
    "        'prompting_method': prompting_method,\n",
    "        'accuracy': accuracy,\n",
    "        'cost': cost\n",
    "    }])\n",
    "\n",
    "    # Append to the file or create a new one\n",
    "    if file_exists:\n",
    "        # Append without writing the header again\n",
    "        result.to_csv(file_path, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        # Write with header if the file is new\n",
    "        result.to_csv(file_path, index=False)\n",
    "\n",
    "# Define your model and method here\n",
    "model_name = \"Meta-Llama-3.1-8B-Instruct\"  # Change to the desired model\n",
    "prompting_method = \"direct\"  # Change to 'direct', 'chain-of-thought', or 'few-shot'\n",
    "\n",
    "# Load dataset and evaluate\n",
    "data, locations = load_nyt_dataset()\n",
    "accuracy, cost = evaluate_model(data, locations, model_name, prompting_method)\n",
    "\n",
    "# Print and save results\n",
    "print(f\"Model: {model_name}, Prompting Method: {prompting_method}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}, Cost: ${cost:.2f}\")\n",
    "\n",
    "results_file = 'nyt_location_result'\n",
    "save_results_to_csv(results_file, model_name, prompting_method, accuracy, cost)\n",
    "print(f\"Results saved to {results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Meta-Llama-3.1-8B-Instruct, Prompting Method: chain-of-thought\n",
      "Accuracy: 0.88, Cost: $0.01\n",
      "Results saved to nyt_location_result\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import openai\n",
    "from openai import RateLimitError, APIError\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.environ.get(\"SAMBANOVA_API_KEY\"),\n",
    "    base_url=\"https://api.sambanova.ai/v1\",\n",
    ")\n",
    "\n",
    "# Token counting using a simple string split method\n",
    "def count_tokens(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# Truncate text to fit within a specified token budget\n",
    "def truncate_text(text, max_tokens, reserve_tokens=50):\n",
    "    tokens = text.split()\n",
    "    max_input_tokens = max_tokens - reserve_tokens\n",
    "    if len(tokens) > max_input_tokens:\n",
    "        tokens = tokens[:max_input_tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Load the NYT dataset\n",
    "def load_nyt_dataset(data_dir='data/nyt_data'):\n",
    "    # File paths\n",
    "    phrase_file = os.path.join(data_dir, 'phrase_text.txt')\n",
    "    locations_file = os.path.join(data_dir, 'locations.txt')\n",
    "    locations_label_file = os.path.join(data_dir, 'locations_label.txt')\n",
    "\n",
    "    # Load phrases\n",
    "    with open(phrase_file, 'r', encoding='utf-8') as f:\n",
    "        phrases = [line.strip() for line in f]\n",
    "\n",
    "    # Load locations and their labels\n",
    "    with open(locations_file, 'r', encoding='utf-8') as f:\n",
    "        locations = [line.strip() for line in f]\n",
    "    with open(locations_label_file, 'r', encoding='utf-8') as f:\n",
    "        location_labels = [int(line.strip()) for line in f]\n",
    "\n",
    "    # Map location labels to location names\n",
    "    location_mapping = {i: name for i, name in enumerate(locations)}\n",
    "\n",
    "    # Combine data into a structured dictionary\n",
    "    data = []\n",
    "    for i, phrase in enumerate(phrases):\n",
    "        location = location_mapping.get(location_labels[i], \"Unknown\")\n",
    "        data.append({\n",
    "            'text': phrase,\n",
    "            'location': location,\n",
    "        })\n",
    "\n",
    "    return data, locations\n",
    "\n",
    "# Select random samples for evaluation\n",
    "def select_samples(data, num_samples_per_class=10):\n",
    "    classes = set(item['location'] for item in data)\n",
    "    selected_data = {cls: [] for cls in classes}\n",
    "\n",
    "    for item in data:\n",
    "        if len(selected_data[item['location']]) < num_samples_per_class:\n",
    "            selected_data[item['location']].append(item)\n",
    "\n",
    "    return selected_data\n",
    "\n",
    "# Prompting methods\n",
    "def direct_prompt(text, classes_formatted):\n",
    "    return (\n",
    "        f\"{text}\\n\\n\"\n",
    "        f\"Classify the above text into one of the following locations: {classes_formatted}.\\n\"\n",
    "        f\"Provide only the location name as your answer.\"\n",
    "    )\n",
    "\n",
    "def chain_of_thought_prompt(text, classes_formatted):\n",
    "    return (\n",
    "        f\"{text}\\n\\n\"\n",
    "        f\"Think step by step to classify the above text into one of the following locations: {classes_formatted}.\\n\"\n",
    "        f\"Provide only the location name as your answer.\"\n",
    "    )\n",
    "\n",
    "def few_shot_prompt(text, examples, classes_formatted):\n",
    "    examples_text = \"\"\n",
    "    for ex_text, ex_label in examples:\n",
    "        examples_text += f\"Text: {ex_text}\\nLabel: {ex_label}\\n\\n\"\n",
    "    return (\n",
    "        f\"{examples_text}\"\n",
    "        f\"Now, classify the following text:\\n\"\n",
    "        f\"Text: {text}\\n\\n\"\n",
    "        f\"Choose the location from the following options: {classes_formatted}.\\n\"\n",
    "        f\"Provide only the location name as your answer.\"\n",
    "    )\n",
    "\n",
    "# Cost estimation with actual prices\n",
    "def estimate_cost(input_tokens, output_tokens, model_name):\n",
    "    cost_per_million = {\n",
    "        'Llama 3.1 8B': (0.10, 0.20),\n",
    "        'Llama 3.1 70B': (0.60, 1.20),\n",
    "        'Llama 3.1 405B': (5.00, 10.00),\n",
    "        'Llama 3.2 1B': (0.04, 0.08),\n",
    "        'Llama 3.2 3B': (0.08, 0.16),\n",
    "        'Llama 3.2 11B Vision': (0.15, 0.30),\n",
    "        'Llama 3.2 90B Vision': (0.80, 1.60),\n",
    "    }\n",
    "    input_price, output_price = cost_per_million.get(model_name, (0.10, 0.20))\n",
    "    total_input_cost = (input_tokens / 1e6) * input_price\n",
    "    total_output_cost = (output_tokens / 1e6) * output_price\n",
    "    return total_input_cost + total_output_cost\n",
    "\n",
    "# Evaluate a single method and model\n",
    "def evaluate_model(\n",
    "    data, locations, model_name, prompting_method,\n",
    "    num_samples_per_class=10, token_budget=4000, delay=2, max_retries=3\n",
    "):\n",
    "    correct_predictions = 0\n",
    "    total_cost = 0\n",
    "    total_samples = 0\n",
    "    classes_formatted = \", \".join(locations)\n",
    "\n",
    "    selected_data = select_samples(data, num_samples_per_class)\n",
    "\n",
    "    for cls, samples in selected_data.items():\n",
    "        for item in samples:\n",
    "            # Truncate text to fit within token budget\n",
    "            text = truncate_text(item['text'], token_budget, reserve_tokens=100)\n",
    "\n",
    "            # Create prompt based on the chosen prompting method\n",
    "            if prompting_method == 'direct':\n",
    "                prompt = direct_prompt(text, classes_formatted)\n",
    "            elif prompting_method == 'chain-of-thought':\n",
    "                prompt = chain_of_thought_prompt(text, classes_formatted)\n",
    "            elif prompting_method == 'few-shot':\n",
    "                examples = random.sample(data, min(3, len(data)))\n",
    "                examples = [(ex['text'], ex['location']) for ex in examples]\n",
    "                prompt = few_shot_prompt(text, examples, classes_formatted)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown prompting method: {prompting_method}\")\n",
    "\n",
    "            input_token_count = count_tokens(prompt)\n",
    "            if input_token_count > token_budget:\n",
    "                print(f\"Skipping sample due to token limit: {input_token_count} tokens\")\n",
    "                continue\n",
    "\n",
    "            retries = 0\n",
    "            while retries <= max_retries:\n",
    "                try:\n",
    "                    # API call\n",
    "                    completion = client.chat.completions.create(\n",
    "                        model=model_name,\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                        stream=True,\n",
    "                        temperature=0.1,\n",
    "                        top_p=0.1\n",
    "                    )\n",
    "                    response_text = \"\".join(\n",
    "                        chunk.choices[0].delta.content or \"\" for chunk in completion\n",
    "                    )\n",
    "\n",
    "                    output_token_count = count_tokens(response_text)\n",
    "                    total_cost += estimate_cost(input_token_count, output_token_count, model_name)\n",
    "\n",
    "                    if response_text.strip().lower() == cls.lower():\n",
    "                        correct_predictions += 1\n",
    "                    break  # Exit retry loop on success\n",
    "\n",
    "                except RateLimitError:\n",
    "                    print(\"Rate limit exceeded. Sleeping for 30 seconds...\")\n",
    "                    time.sleep(30)\n",
    "                    retries += 1\n",
    "\n",
    "                except APIError as e:\n",
    "                    print(f\"API Error: {e}\")\n",
    "                    if \"maximum sequence length\" in str(e):\n",
    "                        print(\"Token limit exceeded. Please adjust the input size.\")\n",
    "                    retries += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Unexpected error: {e}\")\n",
    "                    break\n",
    "\n",
    "            total_samples += 1\n",
    "            time.sleep(delay)  # Respect delay between requests\n",
    "\n",
    "    accuracy = correct_predictions / total_samples if total_samples > 0 else 0\n",
    "    return accuracy, total_cost\n",
    "\n",
    "# Save results to a CSV file\n",
    "def save_results_to_csv(file_path, model_name, prompting_method, accuracy, cost):\n",
    "    # Check if the file exists\n",
    "    file_exists = os.path.isfile(file_path)\n",
    "\n",
    "    # Create a DataFrame with the current results\n",
    "    result = pd.DataFrame([{\n",
    "        'model': model_name,\n",
    "        'prompting_method': prompting_method,\n",
    "        'accuracy': accuracy,\n",
    "        'cost': cost\n",
    "    }])\n",
    "\n",
    "    # Append to the file or create a new one\n",
    "    if file_exists:\n",
    "        # Append without writing the header again\n",
    "        result.to_csv(file_path, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        # Write with header if the file is new\n",
    "        result.to_csv(file_path, index=False)\n",
    "\n",
    "# Define your model and method here\n",
    "model_name = \"Meta-Llama-3.1-8B-Instruct\"  # Change to the desired model\n",
    "prompting_method = \"chain-of-thought\"  # Change to 'direct', 'chain-of-thought', or 'few-shot'\n",
    "\n",
    "# Load dataset and evaluate\n",
    "data, locations = load_nyt_dataset()\n",
    "accuracy, cost = evaluate_model(data, locations, model_name, prompting_method)\n",
    "\n",
    "# Print and save results\n",
    "print(f\"Model: {model_name}, Prompting Method: {prompting_method}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}, Cost: ${cost:.2f}\")\n",
    "\n",
    "results_file = 'nyt_location_result'\n",
    "save_results_to_csv(results_file, model_name, prompting_method, accuracy, cost)\n",
    "print(f\"Results saved to {results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping sample due to token limit: 5970 tokens\n",
      "Skipping sample due to token limit: 4716 tokens\n",
      "Skipping sample due to token limit: 4996 tokens\n",
      "Skipping sample due to token limit: 5762 tokens\n",
      "Skipping sample due to token limit: 4211 tokens\n",
      "Skipping sample due to token limit: 4577 tokens\n",
      "Skipping sample due to token limit: 4251 tokens\n",
      "Skipping sample due to token limit: 4179 tokens\n",
      "Skipping sample due to token limit: 7911 tokens\n",
      "Skipping sample due to token limit: 6328 tokens\n",
      "Skipping sample due to token limit: 4256 tokens\n",
      "Skipping sample due to token limit: 4130 tokens\n",
      "Skipping sample due to token limit: 4574 tokens\n",
      "Skipping sample due to token limit: 6312 tokens\n",
      "Skipping sample due to token limit: 4010 tokens\n",
      "Skipping sample due to token limit: 4429 tokens\n",
      "Skipping sample due to token limit: 7311 tokens\n",
      "Skipping sample due to token limit: 5114 tokens\n",
      "Skipping sample due to token limit: 6350 tokens\n",
      "Skipping sample due to token limit: 4342 tokens\n",
      "Skipping sample due to token limit: 4180 tokens\n",
      "Skipping sample due to token limit: 4092 tokens\n",
      "Skipping sample due to token limit: 5406 tokens\n",
      "Model: Meta-Llama-3.1-8B-Instruct, Prompting Method: few-shot\n",
      "Accuracy: 0.87, Cost: $0.02\n",
      "Results saved to nyt_location_result\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import openai\n",
    "from openai import RateLimitError, APIError\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.environ.get(\"SAMBANOVA_API_KEY\"),\n",
    "    base_url=\"https://api.sambanova.ai/v1\",\n",
    ")\n",
    "\n",
    "# Token counting using a simple string split method\n",
    "def count_tokens(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# Truncate text to fit within a specified token budget\n",
    "def truncate_text(text, max_tokens, reserve_tokens=50):\n",
    "    tokens = text.split()\n",
    "    max_input_tokens = max_tokens - reserve_tokens\n",
    "    if len(tokens) > max_input_tokens:\n",
    "        tokens = tokens[:max_input_tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Load the NYT dataset\n",
    "def load_nyt_dataset(data_dir='data/nyt_data'):\n",
    "    # File paths\n",
    "    phrase_file = os.path.join(data_dir, 'phrase_text.txt')\n",
    "    locations_file = os.path.join(data_dir, 'locations.txt')\n",
    "    locations_label_file = os.path.join(data_dir, 'locations_label.txt')\n",
    "\n",
    "    # Load phrases\n",
    "    with open(phrase_file, 'r', encoding='utf-8') as f:\n",
    "        phrases = [line.strip() for line in f]\n",
    "\n",
    "    # Load locations and their labels\n",
    "    with open(locations_file, 'r', encoding='utf-8') as f:\n",
    "        locations = [line.strip() for line in f]\n",
    "    with open(locations_label_file, 'r', encoding='utf-8') as f:\n",
    "        location_labels = [int(line.strip()) for line in f]\n",
    "\n",
    "    # Map location labels to location names\n",
    "    location_mapping = {i: name for i, name in enumerate(locations)}\n",
    "\n",
    "    # Combine data into a structured dictionary\n",
    "    data = []\n",
    "    for i, phrase in enumerate(phrases):\n",
    "        location = location_mapping.get(location_labels[i], \"Unknown\")\n",
    "        data.append({\n",
    "            'text': phrase,\n",
    "            'location': location,\n",
    "        })\n",
    "\n",
    "    return data, locations\n",
    "\n",
    "# Select random samples for evaluation\n",
    "def select_samples(data, num_samples_per_class=10):\n",
    "    classes = set(item['location'] for item in data)\n",
    "    selected_data = {cls: [] for cls in classes}\n",
    "\n",
    "    for item in data:\n",
    "        if len(selected_data[item['location']]) < num_samples_per_class:\n",
    "            selected_data[item['location']].append(item)\n",
    "\n",
    "    return selected_data\n",
    "\n",
    "# Prompting methods\n",
    "def direct_prompt(text, classes_formatted):\n",
    "    return (\n",
    "        f\"{text}\\n\\n\"\n",
    "        f\"Classify the above text into one of the following locations: {classes_formatted}.\\n\"\n",
    "        f\"Provide only the location name as your answer.\"\n",
    "    )\n",
    "\n",
    "def chain_of_thought_prompt(text, classes_formatted):\n",
    "    return (\n",
    "        f\"{text}\\n\\n\"\n",
    "        f\"Think step by step to classify the above text into one of the following locations: {classes_formatted}.\\n\"\n",
    "        f\"Provide only the location name as your answer.\"\n",
    "    )\n",
    "\n",
    "def few_shot_prompt(text, examples, classes_formatted):\n",
    "    examples_text = \"\"\n",
    "    for ex_text, ex_label in examples:\n",
    "        examples_text += f\"Text: {ex_text}\\nLabel: {ex_label}\\n\\n\"\n",
    "    return (\n",
    "        f\"{examples_text}\"\n",
    "        f\"Now, classify the following text:\\n\"\n",
    "        f\"Text: {text}\\n\\n\"\n",
    "        f\"Choose the location from the following options: {classes_formatted}.\\n\"\n",
    "        f\"Provide only the location name as your answer.\"\n",
    "    )\n",
    "\n",
    "# Cost estimation with actual prices\n",
    "def estimate_cost(input_tokens, output_tokens, model_name):\n",
    "    cost_per_million = {\n",
    "        'Llama 3.1 8B': (0.10, 0.20),\n",
    "        'Llama 3.1 70B': (0.60, 1.20),\n",
    "        'Llama 3.1 405B': (5.00, 10.00),\n",
    "        'Llama 3.2 1B': (0.04, 0.08),\n",
    "        'Llama 3.2 3B': (0.08, 0.16),\n",
    "        'Llama 3.2 11B Vision': (0.15, 0.30),\n",
    "        'Llama 3.2 90B Vision': (0.80, 1.60),\n",
    "    }\n",
    "    input_price, output_price = cost_per_million.get(model_name, (0.10, 0.20))\n",
    "    total_input_cost = (input_tokens / 1e6) * input_price\n",
    "    total_output_cost = (output_tokens / 1e6) * output_price\n",
    "    return total_input_cost + total_output_cost\n",
    "\n",
    "# Evaluate a single method and model\n",
    "def evaluate_model(\n",
    "    data, locations, model_name, prompting_method,\n",
    "    num_samples_per_class=10, token_budget=4000, delay=2, max_retries=3\n",
    "):\n",
    "    correct_predictions = 0\n",
    "    total_cost = 0\n",
    "    total_samples = 0\n",
    "    classes_formatted = \", \".join(locations)\n",
    "\n",
    "    selected_data = select_samples(data, num_samples_per_class)\n",
    "\n",
    "    for cls, samples in selected_data.items():\n",
    "        for item in samples:\n",
    "            # Truncate text to fit within token budget\n",
    "            text = truncate_text(item['text'], token_budget, reserve_tokens=100)\n",
    "\n",
    "            # Create prompt based on the chosen prompting method\n",
    "            if prompting_method == 'direct':\n",
    "                prompt = direct_prompt(text, classes_formatted)\n",
    "            elif prompting_method == 'chain-of-thought':\n",
    "                prompt = chain_of_thought_prompt(text, classes_formatted)\n",
    "            elif prompting_method == 'few-shot':\n",
    "                examples = random.sample(data, min(3, len(data)))\n",
    "                examples = [(ex['text'], ex['location']) for ex in examples]\n",
    "                prompt = few_shot_prompt(text, examples, classes_formatted)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown prompting method: {prompting_method}\")\n",
    "\n",
    "            input_token_count = count_tokens(prompt)\n",
    "            if input_token_count > token_budget:\n",
    "                print(f\"Skipping sample due to token limit: {input_token_count} tokens\")\n",
    "                continue\n",
    "\n",
    "            retries = 0\n",
    "            while retries <= max_retries:\n",
    "                try:\n",
    "                    # API call\n",
    "                    completion = client.chat.completions.create(\n",
    "                        model=model_name,\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                        stream=True,\n",
    "                        temperature=0.1,\n",
    "                        top_p=0.1\n",
    "                    )\n",
    "                    response_text = \"\".join(\n",
    "                        chunk.choices[0].delta.content or \"\" for chunk in completion\n",
    "                    )\n",
    "\n",
    "                    output_token_count = count_tokens(response_text)\n",
    "                    total_cost += estimate_cost(input_token_count, output_token_count, model_name)\n",
    "\n",
    "                    if response_text.strip().lower() == cls.lower():\n",
    "                        correct_predictions += 1\n",
    "                    break  # Exit retry loop on success\n",
    "\n",
    "                except RateLimitError:\n",
    "                    print(\"Rate limit exceeded. Sleeping for 30 seconds...\")\n",
    "                    time.sleep(30)\n",
    "                    retries += 1\n",
    "\n",
    "                except APIError as e:\n",
    "                    print(f\"API Error: {e}\")\n",
    "                    if \"maximum sequence length\" in str(e):\n",
    "                        print(\"Token limit exceeded. Please adjust the input size.\")\n",
    "                    retries += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Unexpected error: {e}\")\n",
    "                    break\n",
    "\n",
    "            total_samples += 1\n",
    "            time.sleep(delay)  # Respect delay between requests\n",
    "\n",
    "    accuracy = correct_predictions / total_samples if total_samples > 0 else 0\n",
    "    return accuracy, total_cost\n",
    "\n",
    "# Save results to a CSV file\n",
    "def save_results_to_csv(file_path, model_name, prompting_method, accuracy, cost):\n",
    "    # Check if the file exists\n",
    "    file_exists = os.path.isfile(file_path)\n",
    "\n",
    "    # Create a DataFrame with the current results\n",
    "    result = pd.DataFrame([{\n",
    "        'model': model_name,\n",
    "        'prompting_method': prompting_method,\n",
    "        'accuracy': accuracy,\n",
    "        'cost': cost\n",
    "    }])\n",
    "\n",
    "    # Append to the file or create a new one\n",
    "    if file_exists:\n",
    "        # Append without writing the header again\n",
    "        result.to_csv(file_path, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        # Write with header if the file is new\n",
    "        result.to_csv(file_path, index=False)\n",
    "\n",
    "# Define your model and method here\n",
    "model_name = \"Meta-Llama-3.1-8B-Instruct\"  # Change to the desired model\n",
    "prompting_method = \"few-shot\"  # Change to 'direct', 'chain-of-thought', or 'few-shot'\n",
    "\n",
    "# Load dataset and evaluate\n",
    "data, locations = load_nyt_dataset()\n",
    "accuracy, cost = evaluate_model(data, locations, model_name, prompting_method)\n",
    "\n",
    "# Print and save results\n",
    "print(f\"Model: {model_name}, Prompting Method: {prompting_method}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}, Cost: ${cost:.2f}\")\n",
    "\n",
    "results_file = 'nyt_location_result'\n",
    "save_results_to_csv(results_file, model_name, prompting_method, accuracy, cost)\n",
    "print(f\"Results saved to {results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
