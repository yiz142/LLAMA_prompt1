{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: ['business', 'politics', 'sports', 'health', 'education', 'real_estate', 'arts', 'science', 'technology']\n",
      "Locations: ['united_states', 'iraq', 'japan', 'china', 'britain', 'russia', 'germany', 'canada', 'france', 'italy']\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4440 tokens long, so generating 1 tokens requires a sequence length of 4441, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4440 tokens long, so generating 1 tokens requires a sequence length of 4441, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4440 tokens long, so generating 1 tokens requires a sequence length of 4441, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4440 tokens long, so generating 1 tokens requires a sequence length of 4441, but the maximum supported sequence length is just 4096!\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4444 tokens long, so generating 1 tokens requires a sequence length of 4445, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4444 tokens long, so generating 1 tokens requires a sequence length of 4445, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4444 tokens long, so generating 1 tokens requires a sequence length of 4445, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4444 tokens long, so generating 1 tokens requires a sequence length of 4445, but the maximum supported sequence length is just 4096!\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Skipping sample due to token limit: 4721 tokens\n",
      "Skipping sample due to token limit: 5588 tokens\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4343 tokens long, so generating 1 tokens requires a sequence length of 4344, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4343 tokens long, so generating 1 tokens requires a sequence length of 4344, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4343 tokens long, so generating 1 tokens requires a sequence length of 4344, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4343 tokens long, so generating 1 tokens requires a sequence length of 4344, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4175 tokens long, so generating 1 tokens requires a sequence length of 4176, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4175 tokens long, so generating 1 tokens requires a sequence length of 4176, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4175 tokens long, so generating 1 tokens requires a sequence length of 4176, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4175 tokens long, so generating 1 tokens requires a sequence length of 4176, but the maximum supported sequence length is just 4096!\n",
      "Skipping sample due to token limit: 4394 tokens\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4369 tokens long, so generating 1 tokens requires a sequence length of 4370, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4369 tokens long, so generating 1 tokens requires a sequence length of 4370, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4369 tokens long, so generating 1 tokens requires a sequence length of 4370, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4369 tokens long, so generating 1 tokens requires a sequence length of 4370, but the maximum supported sequence length is just 4096!\n",
      "Skipping sample due to token limit: 4011 tokens\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Skipping sample due to token limit: 5475 tokens\n",
      "Skipping sample due to token limit: 6886 tokens\n",
      "Skipping sample due to token limit: 4895 tokens\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4138 tokens long, so generating 1 tokens requires a sequence length of 4139, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4138 tokens long, so generating 1 tokens requires a sequence length of 4139, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4138 tokens long, so generating 1 tokens requires a sequence length of 4139, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4138 tokens long, so generating 1 tokens requires a sequence length of 4139, but the maximum supported sequence length is just 4096!\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Skipping sample due to token limit: 4514 tokens\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4417 tokens long, so generating 1 tokens requires a sequence length of 4418, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4417 tokens long, so generating 1 tokens requires a sequence length of 4418, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4417 tokens long, so generating 1 tokens requires a sequence length of 4418, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4417 tokens long, so generating 1 tokens requires a sequence length of 4418, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4196 tokens long, so generating 1 tokens requires a sequence length of 4197, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4196 tokens long, so generating 1 tokens requires a sequence length of 4197, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4196 tokens long, so generating 1 tokens requires a sequence length of 4197, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4196 tokens long, so generating 1 tokens requires a sequence length of 4197, but the maximum supported sequence length is just 4096!\n",
      "Skipping sample due to token limit: 5723 tokens\n",
      "Skipping sample due to token limit: 4108 tokens\n",
      "Skipping sample due to token limit: 4076 tokens\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4135 tokens long, so generating 1 tokens requires a sequence length of 4136, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4135 tokens long, so generating 1 tokens requires a sequence length of 4136, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4135 tokens long, so generating 1 tokens requires a sequence length of 4136, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4135 tokens long, so generating 1 tokens requires a sequence length of 4136, but the maximum supported sequence length is just 4096!\n",
      "Skipping sample due to token limit: 4912 tokens\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4299 tokens long, so generating 1 tokens requires a sequence length of 4300, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4299 tokens long, so generating 1 tokens requires a sequence length of 4300, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4299 tokens long, so generating 1 tokens requires a sequence length of 4300, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4299 tokens long, so generating 1 tokens requires a sequence length of 4300, but the maximum supported sequence length is just 4096!\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4405 tokens long, so generating 1 tokens requires a sequence length of 4406, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4405 tokens long, so generating 1 tokens requires a sequence length of 4406, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4405 tokens long, so generating 1 tokens requires a sequence length of 4406, but the maximum supported sequence length is just 4096!\n",
      "Skipping sample due to token limit: 4649 tokens\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4146 tokens long, so generating 1 tokens requires a sequence length of 4147, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4146 tokens long, so generating 1 tokens requires a sequence length of 4147, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4146 tokens long, so generating 1 tokens requires a sequence length of 4147, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4146 tokens long, so generating 1 tokens requires a sequence length of 4147, but the maximum supported sequence length is just 4096!\n",
      "Skipping sample due to token limit: 5016 tokens\n",
      "Skipping sample due to token limit: 4408 tokens\n",
      "Skipping sample due to token limit: 5438 tokens\n",
      "Skipping sample due to token limit: 4498 tokens\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4143 tokens long, so generating 1 tokens requires a sequence length of 4144, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4143 tokens long, so generating 1 tokens requires a sequence length of 4144, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4143 tokens long, so generating 1 tokens requires a sequence length of 4144, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4143 tokens long, so generating 1 tokens requires a sequence length of 4144, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4191 tokens long, so generating 1 tokens requires a sequence length of 4192, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4191 tokens long, so generating 1 tokens requires a sequence length of 4192, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4191 tokens long, so generating 1 tokens requires a sequence length of 4192, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4191 tokens long, so generating 1 tokens requires a sequence length of 4192, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4134 tokens long, so generating 1 tokens requires a sequence length of 4135, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4134 tokens long, so generating 1 tokens requires a sequence length of 4135, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4134 tokens long, so generating 1 tokens requires a sequence length of 4135, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4134 tokens long, so generating 1 tokens requires a sequence length of 4135, but the maximum supported sequence length is just 4096!\n",
      "Skipping sample due to token limit: 5321 tokens\n",
      "Skipping sample due to token limit: 4263 tokens\n",
      "Skipping sample due to token limit: 4115 tokens\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4282 tokens long, so generating 1 tokens requires a sequence length of 4283, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4282 tokens long, so generating 1 tokens requires a sequence length of 4283, but the maximum supported sequence length is just 4096!\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4382 tokens long, so generating 1 tokens requires a sequence length of 4383, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4382 tokens long, so generating 1 tokens requires a sequence length of 4383, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4382 tokens long, so generating 1 tokens requires a sequence length of 4383, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4382 tokens long, so generating 1 tokens requires a sequence length of 4383, but the maximum supported sequence length is just 4096!\n",
      "Skipping sample due to token limit: 4159 tokens\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4440 tokens long, so generating 1 tokens requires a sequence length of 4441, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4440 tokens long, so generating 1 tokens requires a sequence length of 4441, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4440 tokens long, so generating 1 tokens requires a sequence length of 4441, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4440 tokens long, so generating 1 tokens requires a sequence length of 4441, but the maximum supported sequence length is just 4096!\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4444 tokens long, so generating 1 tokens requires a sequence length of 4445, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4444 tokens long, so generating 1 tokens requires a sequence length of 4445, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4444 tokens long, so generating 1 tokens requires a sequence length of 4445, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4444 tokens long, so generating 1 tokens requires a sequence length of 4445, but the maximum supported sequence length is just 4096!\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Skipping sample due to token limit: 5310 tokens\n",
      "Skipping sample due to token limit: 5301 tokens\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4370 tokens long, so generating 1 tokens requires a sequence length of 4371, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4370 tokens long, so generating 1 tokens requires a sequence length of 4371, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4370 tokens long, so generating 1 tokens requires a sequence length of 4371, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4370 tokens long, so generating 1 tokens requires a sequence length of 4371, but the maximum supported sequence length is just 4096!\n",
      "Skipping sample due to token limit: 4747 tokens\n",
      "Skipping sample due to token limit: 4430 tokens\n",
      "Skipping sample due to token limit: 4127 tokens\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4538 tokens long, so generating 1 tokens requires a sequence length of 4539, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4538 tokens long, so generating 1 tokens requires a sequence length of 4539, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4538 tokens long, so generating 1 tokens requires a sequence length of 4539, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4538 tokens long, so generating 1 tokens requires a sequence length of 4539, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4133 tokens long, so generating 1 tokens requires a sequence length of 4134, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4133 tokens long, so generating 1 tokens requires a sequence length of 4134, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4133 tokens long, so generating 1 tokens requires a sequence length of 4134, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4133 tokens long, so generating 1 tokens requires a sequence length of 4134, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4169 tokens long, so generating 1 tokens requires a sequence length of 4170, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4169 tokens long, so generating 1 tokens requires a sequence length of 4170, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4169 tokens long, so generating 1 tokens requires a sequence length of 4170, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4169 tokens long, so generating 1 tokens requires a sequence length of 4170, but the maximum supported sequence length is just 4096!\n",
      "Skipping sample due to token limit: 5073 tokens\n",
      "Skipping sample due to token limit: 4385 tokens\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4271 tokens long, so generating 1 tokens requires a sequence length of 4272, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4271 tokens long, so generating 1 tokens requires a sequence length of 4272, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4271 tokens long, so generating 1 tokens requires a sequence length of 4272, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4271 tokens long, so generating 1 tokens requires a sequence length of 4272, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4187 tokens long, so generating 1 tokens requires a sequence length of 4188, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4187 tokens long, so generating 1 tokens requires a sequence length of 4188, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4187 tokens long, so generating 1 tokens requires a sequence length of 4188, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4187 tokens long, so generating 1 tokens requires a sequence length of 4188, but the maximum supported sequence length is just 4096!\n",
      "Rate limit exceeded. Sleeping for 30 seconds...\n",
      "Skipping sample due to token limit: 4319 tokens\n",
      "Skipping sample due to token limit: 6396 tokens\n",
      "Skipping sample due to token limit: 5415 tokens\n",
      "Skipping sample due to token limit: 4621 tokens\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4198 tokens long, so generating 1 tokens requires a sequence length of 4199, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4198 tokens long, so generating 1 tokens requires a sequence length of 4199, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4198 tokens long, so generating 1 tokens requires a sequence length of 4199, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4198 tokens long, so generating 1 tokens requires a sequence length of 4199, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4641 tokens long, so generating 1 tokens requires a sequence length of 4642, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4641 tokens long, so generating 1 tokens requires a sequence length of 4642, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4641 tokens long, so generating 1 tokens requires a sequence length of 4642, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4641 tokens long, so generating 1 tokens requires a sequence length of 4642, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4568 tokens long, so generating 1 tokens requires a sequence length of 4569, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4568 tokens long, so generating 1 tokens requires a sequence length of 4569, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4568 tokens long, so generating 1 tokens requires a sequence length of 4569, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4568 tokens long, so generating 1 tokens requires a sequence length of 4569, but the maximum supported sequence length is just 4096!\n",
      "Skipping sample due to token limit: 4806 tokens\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4557 tokens long, so generating 1 tokens requires a sequence length of 4558, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4557 tokens long, so generating 1 tokens requires a sequence length of 4558, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4557 tokens long, so generating 1 tokens requires a sequence length of 4558, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4557 tokens long, so generating 1 tokens requires a sequence length of 4558, but the maximum supported sequence length is just 4096!\n",
      "Skipping sample due to token limit: 4153 tokens\n",
      "Skipping sample due to token limit: 4821 tokens\n",
      "Skipping sample due to token limit: 6629 tokens\n",
      "Skipping sample due to token limit: 4475 tokens\n",
      "Skipping sample due to token limit: 4840 tokens\n",
      "Skipping sample due to token limit: 4681 tokens\n",
      "Skipping sample due to token limit: 6126 tokens\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4315 tokens long, so generating 1 tokens requires a sequence length of 4316, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4315 tokens long, so generating 1 tokens requires a sequence length of 4316, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4315 tokens long, so generating 1 tokens requires a sequence length of 4316, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4315 tokens long, so generating 1 tokens requires a sequence length of 4316, but the maximum supported sequence length is just 4096!\n",
      "Skipping sample due to token limit: 5743 tokens\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4343 tokens long, so generating 1 tokens requires a sequence length of 4344, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4343 tokens long, so generating 1 tokens requires a sequence length of 4344, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4343 tokens long, so generating 1 tokens requires a sequence length of 4344, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4343 tokens long, so generating 1 tokens requires a sequence length of 4344, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4635 tokens long, so generating 1 tokens requires a sequence length of 4636, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4635 tokens long, so generating 1 tokens requires a sequence length of 4636, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4635 tokens long, so generating 1 tokens requires a sequence length of 4636, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4635 tokens long, so generating 1 tokens requires a sequence length of 4636, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4819 tokens long, so generating 1 tokens requires a sequence length of 4820, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4819 tokens long, so generating 1 tokens requires a sequence length of 4820, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4819 tokens long, so generating 1 tokens requires a sequence length of 4820, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4819 tokens long, so generating 1 tokens requires a sequence length of 4820, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4487 tokens long, so generating 1 tokens requires a sequence length of 4488, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4487 tokens long, so generating 1 tokens requires a sequence length of 4488, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4487 tokens long, so generating 1 tokens requires a sequence length of 4488, but the maximum supported sequence length is just 4096!\n",
      "API Error: Requested generation length 1 is not possible! The provided prompt is 4487 tokens long, so generating 1 tokens requires a sequence length of 4488, but the maximum supported sequence length is just 4096!\n",
      "Skipping sample due to token limit: 4323 tokens\n",
      "Skipping sample due to token limit: 5234 tokens\n",
      "Skipping sample due to token limit: 4030 tokens\n",
      "Skipping sample due to token limit: 4478 tokens\n",
      "Skipping sample due to token limit: 4231 tokens\n",
      "Skipping sample due to token limit: 4855 tokens\n",
      "Skipping sample due to token limit: 12482 tokens\n",
      "Skipping sample due to token limit: 5678 tokens\n",
      "Skipping sample due to token limit: 5457 tokens\n",
      "Skipping sample due to token limit: 5668 tokens\n",
      "Skipping sample due to token limit: 4876 tokens\n",
      "Skipping sample due to token limit: 4086 tokens\n",
      "Skipping sample due to token limit: 5680 tokens\n",
      "Skipping sample due to token limit: 4135 tokens\n",
      "Skipping sample due to token limit: 4081 tokens\n",
      "Skipping sample due to token limit: 4400 tokens\n",
      "Skipping sample due to token limit: 4341 tokens\n",
      "Skipping sample due to token limit: 6046 tokens\n",
      "Skipping sample due to token limit: 5994 tokens\n",
      "Skipping sample due to token limit: 4522 tokens\n",
      "Skipping sample due to token limit: 8602 tokens\n",
      "Skipping sample due to token limit: 4607 tokens\n",
      "                        model  prompting_method  accuracy     cost\n",
      "0  Meta-Llama-3.2-1B-Instruct            direct  0.300000   7.9491\n",
      "1  Meta-Llama-3.2-1B-Instruct  chain-of-thought  0.322222   7.9936\n",
      "2  Meta-Llama-3.2-1B-Instruct          few-shot  0.231884  13.3214\n",
      "3  Meta-Llama-3.2-3B-Instruct            direct  0.788889   7.9490\n",
      "4  Meta-Llama-3.2-3B-Instruct  chain-of-thought  0.788889   7.9935\n",
      "5  Meta-Llama-3.2-3B-Instruct          few-shot  0.642857  13.3786\n",
      "6  Meta-Llama-3.1-8B-Instruct            direct  0.800000   8.3418\n",
      "7  Meta-Llama-3.1-8B-Instruct  chain-of-thought  0.788889   8.3868\n",
      "8  Meta-Llama-3.1-8B-Instruct          few-shot  0.794118  19.0632\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import openai\n",
    "from openai import RateLimitError, APIError\n",
    "\n",
    "\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.environ.get(\"SAMBANOVA_API_KEY\"),\n",
    "    base_url=\"https://api.sambanova.ai/v1\",\n",
    ")\n",
    "\n",
    "\n",
    "# Token counting using a simple string split method\n",
    "def count_tokens(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# Truncate text to fit within a specified token budget\n",
    "def truncate_text(text, max_tokens, reserve_tokens=50):\n",
    "    \"\"\"\n",
    "    Truncate text to fit within the token budget.\n",
    "    reserve_tokens: Number of tokens reserved for the output.\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    max_input_tokens = max_tokens - reserve_tokens\n",
    "    if len(tokens) > max_input_tokens:\n",
    "        tokens = tokens[:max_input_tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Load the NYT dataset\n",
    "def load_nyt_dataset(data_dir='data/nyt_data'):\n",
    "    # File paths\n",
    "    phrase_file = os.path.join(data_dir, 'phrase_text.txt')\n",
    "    topics_file = os.path.join(data_dir, 'topics.txt')\n",
    "    topics_label_file = os.path.join(data_dir, 'topics_label.txt')\n",
    "    locations_file = os.path.join(data_dir, 'locations.txt')\n",
    "    locations_label_file = os.path.join(data_dir, 'locations_label.txt')\n",
    "    combined_label_file = os.path.join(data_dir, 'label.txt')\n",
    "\n",
    "    # Load phrases\n",
    "    with open(phrase_file, 'r', encoding='utf-8') as f:\n",
    "        phrases = [line.strip() for line in f]\n",
    "\n",
    "    # Load topics and their labels\n",
    "    with open(topics_file, 'r', encoding='utf-8') as f:\n",
    "        topics = [line.strip() for line in f]\n",
    "    topic_labels = []\n",
    "    with open(topics_label_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            stripped_line = line.strip()\n",
    "            if stripped_line.isdigit():  # Check if the line is a valid integer\n",
    "                topic_labels.append(int(stripped_line))\n",
    "            else:\n",
    "                print(f\"Warning: Skipping invalid line in topics_label.txt: '{line.strip()}'\")\n",
    "\n",
    "    # Map topic labels to topic names\n",
    "    topic_mapping = {i: name for i, name in enumerate(topics)}\n",
    "\n",
    "    # Load locations and their labels\n",
    "    with open(locations_file, 'r', encoding='utf-8') as f:\n",
    "        locations = [line.strip() for line in f]\n",
    "    with open(locations_label_file, 'r', encoding='utf-8') as f:\n",
    "        location_labels = [int(line.strip()) for line in f]\n",
    "\n",
    "    # Map location labels to location names\n",
    "    location_mapping = {i: name for i, name in enumerate(locations)}\n",
    "\n",
    "    # Load combined labels\n",
    "    with open(combined_label_file, 'r', encoding='utf-8') as f:\n",
    "        combined_labels = [line.strip().split('\\t') for line in f]\n",
    "\n",
    "    # Combine data into a structured dictionary\n",
    "    data = []\n",
    "    for i, phrase in enumerate(phrases):\n",
    "        topic = topic_mapping.get(topic_labels[i], \"Unknown\")\n",
    "        location = location_mapping.get(location_labels[i], \"Unknown\")\n",
    "        combined_label = combined_labels[i] if i < len(combined_labels) else (\"Unknown\", \"Unknown\")\n",
    "        data.append({\n",
    "            'text': phrase,\n",
    "            'topic': topic,\n",
    "            'location': location,\n",
    "            'combined_label': combined_label\n",
    "        })\n",
    "\n",
    "    return data, topics, locations\n",
    "\n",
    "# Select random samples for evaluation\n",
    "def select_samples(data, num_samples_per_class=10):\n",
    "    classes = set(item['topic'] for item in data)\n",
    "    selected_data = {cls: [] for cls in classes}\n",
    "\n",
    "    for item in data:\n",
    "        if len(selected_data[item['topic']]) < num_samples_per_class:\n",
    "            selected_data[item['topic']].append(item)\n",
    "\n",
    "    return selected_data\n",
    "\n",
    "# Prompting methods\n",
    "def direct_prompt(text, classes_formatted):\n",
    "    return (\n",
    "        f\"{text}\\n\\n\"\n",
    "        f\"Classify the above text into one of the following categories: {classes_formatted}.\\n\"\n",
    "        f\"Provide only the category name as your answer.\"\n",
    "    )\n",
    "\n",
    "def chain_of_thought_prompt(text, classes_formatted):\n",
    "    return (\n",
    "        f\"{text}\\n\\n\"\n",
    "        f\"Think step by step to classify the above text into one of the following categories: {classes_formatted}.\\n\"\n",
    "        f\"Provide only the category name as your answer.\"\n",
    "    )\n",
    "\n",
    "def few_shot_prompt(text, examples, classes_formatted):\n",
    "    examples_text = \"\"\n",
    "    for ex_text, ex_label in examples:\n",
    "        examples_text += f\"Text: {ex_text}\\nLabel: {ex_label}\\n\\n\"\n",
    "    return (\n",
    "        f\"{examples_text}\"\n",
    "        f\"Now, classify the following text:\\n\"\n",
    "        f\"Text: {text}\\n\\n\"\n",
    "        f\"Choose the category from the following options: {classes_formatted}.\\n\"\n",
    "        f\"Provide only the category name as your answer.\"\n",
    "    )\n",
    "\n",
    "# Estimate cost (adjust cost values as necessary)\n",
    "def estimate_cost(input_tokens, output_tokens, model_name):\n",
    "    cost_per_token = {\n",
    "        'Meta-Llama-1B-Instruct': 0.0001,\n",
    "        'Meta-Llama-3B-Instruct': 0.0003,\n",
    "        'Meta-Llama-8B-Instruct': 0.0008,\n",
    "    }\n",
    "    total_tokens = input_tokens + output_tokens\n",
    "    return total_tokens * cost_per_token.get(model_name, 0.0001)\n",
    "\n",
    "def evaluate_model(\n",
    "    data, topics, model_name, prompting_method,\n",
    "    num_samples_per_class=10, token_budget=4000, delay=2, max_retries=3\n",
    "):\n",
    "    correct_predictions = 0\n",
    "    total_cost = 0\n",
    "    total_samples = 0\n",
    "    classes_formatted = \", \".join(topics)\n",
    "\n",
    "    selected_data = select_samples(data, num_samples_per_class)\n",
    "\n",
    "    for cls, samples in selected_data.items():\n",
    "        for item in samples:\n",
    "            # Truncate text to fit within token budget\n",
    "            text = truncate_text(item['text'], token_budget, reserve_tokens=100)\n",
    "\n",
    "            # Create prompt based on the chosen prompting method\n",
    "            if prompting_method == 'direct':\n",
    "                prompt = direct_prompt(text, classes_formatted)\n",
    "            elif prompting_method == 'chain-of-thought':\n",
    "                prompt = chain_of_thought_prompt(text, classes_formatted)\n",
    "            elif prompting_method == 'few-shot':\n",
    "                examples = random.sample(data, min(3, len(data)))\n",
    "                examples = [(ex['text'], ex['topic']) for ex in examples]\n",
    "                prompt = few_shot_prompt(text, examples, classes_formatted)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown prompting method: {prompting_method}\")\n",
    "\n",
    "            input_token_count = count_tokens(prompt)\n",
    "            if input_token_count > token_budget:\n",
    "                print(f\"Skipping sample due to token limit: {input_token_count} tokens\")\n",
    "                continue\n",
    "\n",
    "            retries = 0\n",
    "            while retries <= max_retries:\n",
    "                try:\n",
    "                    # API call\n",
    "                    completion = client.chat.completions.create(\n",
    "                        model=model_name,\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                        stream=True,\n",
    "                        temperature=0.1,\n",
    "                        top_p=0.1\n",
    "                    )\n",
    "                    response_text = \"\".join(\n",
    "                        chunk.choices[0].delta.content or \"\" for chunk in completion\n",
    "                    )\n",
    "\n",
    "                    output_token_count = count_tokens(response_text)\n",
    "                    total_cost += estimate_cost(input_token_count, output_token_count, model_name)\n",
    "\n",
    "                    if response_text.strip().lower() == cls.lower():\n",
    "                        correct_predictions += 1\n",
    "                    break  # Exit retry loop on success\n",
    "\n",
    "                except RateLimitError:\n",
    "                    print(\"Rate limit exceeded. Sleeping for 30 seconds...\")\n",
    "                    time.sleep(30)\n",
    "                    retries += 1\n",
    "\n",
    "                except APIError as e:\n",
    "                    print(f\"API Error: {e}\")\n",
    "                    if \"maximum sequence length\" in str(e):\n",
    "                        print(\"Token limit exceeded. Please adjust the input size.\")\n",
    "                    retries += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Unexpected error: {e}\")\n",
    "                    break\n",
    "\n",
    "            total_samples += 1\n",
    "            time.sleep(delay)  # Respect delay between requests\n",
    "\n",
    "    accuracy = correct_predictions / total_samples if total_samples > 0 else 0\n",
    "    return accuracy, total_cost\n",
    "\n",
    "\n",
    "# Run experiments\n",
    "if __name__ == \"__main__\":\n",
    "    data, topics, locations = load_nyt_dataset()\n",
    "    print(f\"Topics: {topics}\")\n",
    "    print(f\"Locations: {locations}\")\n",
    "\n",
    "    models = ['Meta-Llama-3.2-1B-Instruct', 'Meta-Llama-3.2-3B-Instruct', 'Meta-Llama-3.1-8B-Instruct']\n",
    "    prompting_methods = ['direct', 'chain-of-thought', 'few-shot']\n",
    "\n",
    "    results = []\n",
    "    for model_name in models:\n",
    "        for method in prompting_methods:\n",
    "            accuracy, cost = evaluate_model(data, topics, model_name, method)\n",
    "            results.append({\n",
    "                'model': model_name,\n",
    "                'prompting_method': method,\n",
    "                'accuracy': accuracy,\n",
    "                'cost': cost\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv('nyt_results.csv', index=False)\n",
    "    print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
